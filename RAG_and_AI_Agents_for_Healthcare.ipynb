{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "8cfe3ox3vq-p"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "624af38881d6414dad5a9c1cfac60af5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6f4354752bea43b1a51cb8e7a31f42d1",
              "IPY_MODEL_25e25d892910466d9c09c350b5613770",
              "IPY_MODEL_492762651a6140dfb99e80361d4a64ec"
            ],
            "layout": "IPY_MODEL_c0d3a52ce0974913a7b8921cab6b42b8"
          }
        },
        "6f4354752bea43b1a51cb8e7a31f42d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_63d3eded03ac4babb9b717e41cf417d4",
            "placeholder": "​",
            "style": "IPY_MODEL_22e2f1b735af445085c2562585882c01",
            "value": "Fetching 5 files: 100%"
          }
        },
        "25e25d892910466d9c09c350b5613770": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6cede6e114a6464aa921957c2d617bb6",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c77f9b882188477f8b4cfa57dc380227",
            "value": 5
          }
        },
        "492762651a6140dfb99e80361d4a64ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2f4386e18d044cb496b832defa8e7cfd",
            "placeholder": "​",
            "style": "IPY_MODEL_0a48cb63e7534630907c17854f605b04",
            "value": " 5/5 [00:00&lt;00:00, 211.52it/s]"
          }
        },
        "c0d3a52ce0974913a7b8921cab6b42b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "63d3eded03ac4babb9b717e41cf417d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22e2f1b735af445085c2562585882c01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6cede6e114a6464aa921957c2d617bb6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c77f9b882188477f8b4cfa57dc380227": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2f4386e18d044cb496b832defa8e7cfd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a48cb63e7534630907c17854f605b04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Q1wY3A94Qo4Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nwOUEgUKNZDM",
        "outputId": "93f23d7f-49f3-4b70-cd47-acf66539c0f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.8-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (6.0.2)\n",
            "Collecting SQLAlchemy<2.0.36,>=1.4 (from langchain-community)\n",
            "  Downloading SQLAlchemy-2.0.35-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (3.11.2)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting httpx-sse<0.5.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting langchain<0.4.0,>=0.3.8 (from langchain-community)\n",
            "  Downloading langchain-0.3.9-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting langchain-core<0.4.0,>=0.3.21 (from langchain-community)\n",
            "  Downloading langchain_core-0.3.21-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.1.143)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (1.26.4)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.6.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (9.0.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.17.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.23.1-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.8->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.8->langchain-community) (2.9.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.21->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.21->langchain-community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.21->langchain-community) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-community) (0.27.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-community) (3.10.11)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-community) (1.0.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<2.0.36,>=1.4->langchain-community) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (1.0.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.21->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.8->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.8->langchain-community) (2.23.4)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (1.2.2)\n",
            "Downloading langchain_community-0.3.8-py3-none-any.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading langchain-0.3.9-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.21-py3-none-any.whl (409 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m409.5/409.5 kB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.6.1-py3-none-any.whl (28 kB)\n",
            "Downloading SQLAlchemy-2.0.35-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m83.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.23.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: SQLAlchemy, python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-core, langchain, langchain-community\n",
            "  Attempting uninstall: SQLAlchemy\n",
            "    Found existing installation: SQLAlchemy 2.0.36\n",
            "    Uninstalling SQLAlchemy-2.0.36:\n",
            "      Successfully uninstalled SQLAlchemy-2.0.36\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.19\n",
            "    Uninstalling langchain-core-0.3.19:\n",
            "      Successfully uninstalled langchain-core-0.3.19\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.3.7\n",
            "    Uninstalling langchain-0.3.7:\n",
            "      Successfully uninstalled langchain-0.3.7\n",
            "Successfully installed SQLAlchemy-2.0.35 dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-0.3.9 langchain-community-0.3.8 langchain-core-0.3.21 marshmallow-3.23.1 mypy-extensions-1.0.0 pydantic-settings-2.6.1 python-dotenv-1.0.1 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain-community"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip show langchain-community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hhZCFJU5Q0_z",
        "outputId": "1731d45d-c607-47b2-f371-14dfbc14c704"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: langchain-community\n",
            "Version: 0.3.8\n",
            "Summary: Community contributed LangChain integrations.\n",
            "Home-page: https://github.com/langchain-ai/langchain\n",
            "Author: \n",
            "Author-email: \n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: aiohttp, dataclasses-json, httpx-sse, langchain, langchain-core, langsmith, numpy, pydantic-settings, PyYAML, requests, SQLAlchemy, tenacity\n",
            "Required-by: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import TextLoader"
      ],
      "metadata": {
        "id": "VzOcMdxkRAOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader =TextLoader('nlp.txt')"
      ],
      "metadata": {
        "id": "mrmclB_DRSV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = loader.load()"
      ],
      "metadata": {
        "id": "Us2y5RuSRjrx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M29IRmC6Rw29",
        "outputId": "bd6f57c4-a1d2-48f8-a233-f26fa733c660"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "UFj9q1BcSpkv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df= pd.read_csv('/content/sample_data/california_housing_test.csv')"
      ],
      "metadata": {
        "id": "2O6pWcLkSsJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "moOjn5ubSzsO",
        "outputId": "a94d630e-37ba-43d2-d0f7-11d315025d5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pandas.core.frame.DataFrame"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>pandas.core.frame.DataFrame</b><br/>def __init__(data=None, index: Axes | None=None, columns: Axes | None=None, dtype: Dtype | None=None, copy: bool | None=None) -&gt; None</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py</a>Two-dimensional, size-mutable, potentially heterogeneous tabular data.\n",
              "\n",
              "Data structure also contains labeled axes (rows and columns).\n",
              "Arithmetic operations align on both row and column labels. Can be\n",
              "thought of as a dict-like container for Series objects. The primary\n",
              "pandas data structure.\n",
              "\n",
              "Parameters\n",
              "----------\n",
              "data : ndarray (structured or homogeneous), Iterable, dict, or DataFrame\n",
              "    Dict can contain Series, arrays, constants, dataclass or list-like objects. If\n",
              "    data is a dict, column order follows insertion-order. If a dict contains Series\n",
              "    which have an index defined, it is aligned by its index. This alignment also\n",
              "    occurs if data is a Series or a DataFrame itself. Alignment is done on\n",
              "    Series/DataFrame inputs.\n",
              "\n",
              "    If data is a list of dicts, column order follows insertion-order.\n",
              "\n",
              "index : Index or array-like\n",
              "    Index to use for resulting frame. Will default to RangeIndex if\n",
              "    no indexing information part of input data and no index provided.\n",
              "columns : Index or array-like\n",
              "    Column labels to use for resulting frame when data does not have them,\n",
              "    defaulting to RangeIndex(0, 1, 2, ..., n). If data contains column labels,\n",
              "    will perform column selection instead.\n",
              "dtype : dtype, default None\n",
              "    Data type to force. Only a single dtype is allowed. If None, infer.\n",
              "copy : bool or None, default None\n",
              "    Copy data from inputs.\n",
              "    For dict data, the default of None behaves like ``copy=True``.  For DataFrame\n",
              "    or 2d ndarray input, the default of None behaves like ``copy=False``.\n",
              "    If data is a dict containing one or more Series (possibly of different dtypes),\n",
              "    ``copy=False`` will ensure that these inputs are not copied.\n",
              "\n",
              "    .. versionchanged:: 1.3.0\n",
              "\n",
              "See Also\n",
              "--------\n",
              "DataFrame.from_records : Constructor from tuples, also record arrays.\n",
              "DataFrame.from_dict : From dicts of Series, arrays, or dicts.\n",
              "read_csv : Read a comma-separated values (csv) file into DataFrame.\n",
              "read_table : Read general delimited file into DataFrame.\n",
              "read_clipboard : Read text from clipboard into DataFrame.\n",
              "\n",
              "Notes\n",
              "-----\n",
              "Please reference the :ref:`User Guide &lt;basics.dataframe&gt;` for more information.\n",
              "\n",
              "Examples\n",
              "--------\n",
              "Constructing DataFrame from a dictionary.\n",
              "\n",
              "&gt;&gt;&gt; d = {&#x27;col1&#x27;: [1, 2], &#x27;col2&#x27;: [3, 4]}\n",
              "&gt;&gt;&gt; df = pd.DataFrame(data=d)\n",
              "&gt;&gt;&gt; df\n",
              "   col1  col2\n",
              "0     1     3\n",
              "1     2     4\n",
              "\n",
              "Notice that the inferred dtype is int64.\n",
              "\n",
              "&gt;&gt;&gt; df.dtypes\n",
              "col1    int64\n",
              "col2    int64\n",
              "dtype: object\n",
              "\n",
              "To enforce a single dtype:\n",
              "\n",
              "&gt;&gt;&gt; df = pd.DataFrame(data=d, dtype=np.int8)\n",
              "&gt;&gt;&gt; df.dtypes\n",
              "col1    int8\n",
              "col2    int8\n",
              "dtype: object\n",
              "\n",
              "Constructing DataFrame from a dictionary including Series:\n",
              "\n",
              "&gt;&gt;&gt; d = {&#x27;col1&#x27;: [0, 1, 2, 3], &#x27;col2&#x27;: pd.Series([2, 3], index=[2, 3])}\n",
              "&gt;&gt;&gt; pd.DataFrame(data=d, index=[0, 1, 2, 3])\n",
              "   col1  col2\n",
              "0     0   NaN\n",
              "1     1   NaN\n",
              "2     2   2.0\n",
              "3     3   3.0\n",
              "\n",
              "Constructing DataFrame from numpy ndarray:\n",
              "\n",
              "&gt;&gt;&gt; df2 = pd.DataFrame(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]),\n",
              "...                    columns=[&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;])\n",
              "&gt;&gt;&gt; df2\n",
              "   a  b  c\n",
              "0  1  2  3\n",
              "1  4  5  6\n",
              "2  7  8  9\n",
              "\n",
              "Constructing DataFrame from a numpy ndarray that has labeled columns:\n",
              "\n",
              "&gt;&gt;&gt; data = np.array([(1, 2, 3), (4, 5, 6), (7, 8, 9)],\n",
              "...                 dtype=[(&quot;a&quot;, &quot;i4&quot;), (&quot;b&quot;, &quot;i4&quot;), (&quot;c&quot;, &quot;i4&quot;)])\n",
              "&gt;&gt;&gt; df3 = pd.DataFrame(data, columns=[&#x27;c&#x27;, &#x27;a&#x27;])\n",
              "...\n",
              "&gt;&gt;&gt; df3\n",
              "   c  a\n",
              "0  3  1\n",
              "1  6  4\n",
              "2  9  7\n",
              "\n",
              "Constructing DataFrame from dataclass:\n",
              "\n",
              "&gt;&gt;&gt; from dataclasses import make_dataclass\n",
              "&gt;&gt;&gt; Point = make_dataclass(&quot;Point&quot;, [(&quot;x&quot;, int), (&quot;y&quot;, int)])\n",
              "&gt;&gt;&gt; pd.DataFrame([Point(0, 0), Point(0, 3), Point(2, 3)])\n",
              "   x  y\n",
              "0  0  0\n",
              "1  0  3\n",
              "2  2  3\n",
              "\n",
              "Constructing DataFrame from Series/DataFrame:\n",
              "\n",
              "&gt;&gt;&gt; ser = pd.Series([1, 2, 3], index=[&quot;a&quot;, &quot;b&quot;, &quot;c&quot;])\n",
              "&gt;&gt;&gt; df = pd.DataFrame(data=ser, index=[&quot;a&quot;, &quot;c&quot;])\n",
              "&gt;&gt;&gt; df\n",
              "   0\n",
              "a  1\n",
              "c  3\n",
              "\n",
              "&gt;&gt;&gt; df1 = pd.DataFrame([1, 2, 3], index=[&quot;a&quot;, &quot;b&quot;, &quot;c&quot;], columns=[&quot;x&quot;])\n",
              "&gt;&gt;&gt; df2 = pd.DataFrame(data=df1, index=[&quot;a&quot;, &quot;c&quot;])\n",
              "&gt;&gt;&gt; df2\n",
              "   x\n",
              "a  1\n",
              "c  3</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 509);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.iloc[10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "id": "_mjL1fXeTUhM",
        "outputId": "eca0117b-59ec-49e3-f1a3-577e218ab773"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "longitude               -118.2400\n",
              "latitude                  33.9800\n",
              "housing_median_age        45.0000\n",
              "total_rooms              972.0000\n",
              "total_bedrooms           249.0000\n",
              "population              1288.0000\n",
              "households               261.0000\n",
              "median_income              2.2054\n",
              "median_house_value    125000.0000\n",
              "Name: 10, dtype: float64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>10</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>longitude</th>\n",
              "      <td>-118.2400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>latitude</th>\n",
              "      <td>33.9800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>housing_median_age</th>\n",
              "      <td>45.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>total_rooms</th>\n",
              "      <td>972.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>total_bedrooms</th>\n",
              "      <td>249.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>population</th>\n",
              "      <td>1288.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>households</th>\n",
              "      <td>261.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>median_income</th>\n",
              "      <td>2.2054</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>median_house_value</th>\n",
              "      <td>125000.0000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> float64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYHin_yfRle6",
        "outputId": "295f8e0f-4fb9-4651-de2d-08bb9c6edf56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': 'nlp.txt'}, page_content='Natural language processing has its roots in the 1950s.[1] Already in 1950, Alan Turing published an article titled \"Computing Machinery and Intelligence\" which proposed what is now called the Turing test as a criterion of intelligence, though at the time that was not articulated as a problem separate from artificial intelligence. The proposed test includes a task that involves the automated interpretation and generation of natural language.\\n\\nSymbolic NLP (1950s – early 1990s)\\nThe premise of symbolic NLP is well-summarized by John Searle\\'s Chinese room experiment: Given a collection of rules (e.g., a Chinese phrasebook, with questions and matching answers), the computer emulates natural language understanding (or other NLP tasks) by applying those rules to the data it confronts.\\n\\n1950s: The Georgetown experiment in 1954 involved fully automatic translation of more than sixty Russian sentences into English. The authors claimed that within three or five years, machine translation would be a solved problem.[2] However, real progress was much slower, and after the ALPAC report in 1966, which found that ten years of research had failed to fulfill the expectations, funding for machine translation was dramatically reduced. Little further research in machine translation was conducted in America (though some research continued elsewhere, such as Japan and Europe[3]) until the late 1980s when the first statistical machine translation systems were developed.\\n1960s: Some notably successful natural language processing systems developed in the 1960s were SHRDLU, a natural language system working in restricted \"blocks worlds\" with restricted vocabularies, and ELIZA, a simulation of a Rogerian psychotherapist, written by Joseph Weizenbaum between 1964 and 1966. Using almost no information about human thought or emotion, ELIZA sometimes provided a startlingly human-like interaction. When the \"patient\" exceeded the very small knowledge base, ELIZA might provide a generic response, for example, responding to \"My head hurts\" with \"Why do you say your head hurts?\". Ross Quillian\\'s successful work on natural language was demonstrated with a vocabulary of only twenty words, because that was all that would fit in a computer memory at the time.[4]\\n1970s: During the 1970s, many programmers began to write \"conceptual ontologies\", which structured real-world information into computer-understandable data. Examples are MARGIE (Schank, 1975), SAM (Cullingford, 1978), PAM (Wilensky, 1978), TaleSpin (Meehan, 1976), QUALM (Lehnert, 1977), Politics (Carbonell, 1979), and Plot Units (Lehnert 1981). During this time, the first chatterbots were written (e.g., PARRY).\\n1980s: The 1980s and early 1990s mark the heyday of symbolic methods in NLP. Focus areas of the time included research on rule-based parsing (e.g., the development of HPSG as a computational operationalization of generative grammar), morphology (e.g., two-level morphology[5]), semantics (e.g., Lesk algorithm), reference (e.g., within Centering Theory[6]) and other areas of natural language understanding (e.g., in the Rhetorical Structure Theory). Other lines of research were continued, e.g., the development of chatterbots with Racter and Jabberwacky. An important development (that eventually led to the statistical turn in the 1990s) was the rising importance of quantitative evaluation in this period.[7]\\nStatistical NLP (1990s–2010s)\\nUp until the 1980s, most natural language processing systems were based on complex sets of hand-written rules. Starting in the late 1980s, however, there was a revolution in natural language processing with the introduction of machine learning algorithms for language processing. This was due to both the steady increase in computational power (see Moore\\'s law) and the gradual lessening of the dominance of Chomskyan theories of linguistics (e.g. transformational grammar), whose theoretical underpinnings discouraged the sort of corpus linguistics that underlies the machine-learning approach to language processing.[8]\\n\\n1990s: Many of the notable early successes in statistical methods in NLP occurred in the field of machine translation, due especially to work at IBM Research, such as IBM alignment models. These systems were able to take advantage of existing multilingual textual corpora that had been produced by the Parliament of Canada and the European Union as a result of laws calling for the translation of all governmental proceedings into all official languages of the corresponding systems of government. However, most other systems depended on corpora specifically developed for the tasks implemented by these systems, which was (and often continues to be) a major limitation in the success of these systems. As a result, a great deal of research has gone into methods of more effectively learning from limited amounts of data.\\n2000s: With the growth of the web, increasing amounts of raw (unannotated) language data have become available since the mid-1990s. Research has thus increasingly focused on unsupervised and semi-supervised learning algorithms. Such algorithms can learn from data that has not been hand-annotated with the desired answers or using a combination of annotated and non-annotated data. Generally, this task is much more difficult than supervised learning, and typically produces less accurate results for a given amount of input data. However, there is an enormous amount of non-annotated data available (including, among other things, the entire content of the World Wide Web), which can often make up for the inferior results if the algorithm used has a low enough time complexity to be practical.\\nNeural NLP (present)\\nIn 2003, word n-gram model, at the time the best statistical algorithm, was outperformed by a multi-layer perceptron (with a single hidden layer and context length of several words trained on up to 14 million of words with a CPU cluster in language modelling) by Yoshua Bengio with co-authors.[9]\\n\\nIn 2010, Tomáš Mikolov (then a PhD student at Brno University of Technology) with co-authors applied a simple recurrent neural network with a single hidden layer to language modelling,[10] and in the following years he went on to develop Word2vec. In the 2010s, representation learning and deep neural network-style (featuring many hidden layers) machine learning methods became widespread in natural language processing. That popularity was due partly to a flurry of results showing that such techniques[11][12] can achieve state-of-the-art results in many natural language tasks, e.g., in language modeling[13] and parsing.[14][15] This is increasingly important in medicine and healthcare, where NLP helps analyze notes and text in electronic health records that would otherwise be inaccessible for study when seeking to improve care[16] or protect patient privacy.[17]\\n\\nApproaches: Symbolic, statistical, neural networks\\nSymbolic approach, i.e., the hand-coding of a set of rules for manipulating symbols, coupled with a dictionary lookup, was historically the first approach used both by AI in general and by NLP in particular:[18][19] such as by writing grammars or devising heuristic rules for stemming.\\n\\nMachine learning approaches, which include both statistical and neural networks, on the other hand, have many advantages over the symbolic approach:\\n\\nboth statistical and neural networks methods can focus more on the most common cases extracted from a corpus of texts, whereas the rule-based approach needs to provide rules for both rare cases and common ones equally.\\nlanguage models, produced by either statistical or neural networks methods, are more robust to both unfamiliar (e.g. containing words or structures that have not been seen before) and erroneous input (e.g. with misspelled words or words accidentally omitted) in comparison to the rule-based systems, which are also more costly to produce.\\nthe larger such a (probabilistic) language model is, the more accurate it becomes, in contrast to rule-based systems that can gain accuracy only by increasing the amount and complexity of the rules leading to intractability problems.\\nAlthough rule-based systems for manipulating symbols were still in use in 2020, they have become mostly obsolete with the advance of LLMs in 2023.\\n\\nBefore that they were commonly used:\\n\\nwhen the amount of training data is insufficient to successfully apply machine learning methods, e.g., for the machine translation of low-resource languages such as provided by the Apertium system,\\nfor preprocessing in NLP pipelines, e.g., tokenization, or\\nfor postprocessing and transforming the output of NLP pipelines, e.g., for knowledge extraction from syntactic parses.\\nStatistical approach\\nIn the late 1980s and mid-1990s, the statistical approach ended a period of AI winter, which was caused by the inefficiencies of the rule-based approaches.[20][21]\\n\\nThe earliest decision trees, producing systems of hard if–then rules, were still very similar to the old rule-based approaches. Only the introduction of hidden Markov models, applied to part-of-speech tagging, announced the end of the old rule-based approach.\\n\\nNeural networks\\nFurther information: Artificial neural network\\nA major drawback of statistical methods is that they require elaborate feature engineering. Since 2015,[22] the statistical approach has been replaced by the neural networks approach, using semantic networks[23] and word embeddings to capture semantic properties of words.\\n\\nIntermediate tasks (e.g., part-of-speech tagging and dependency parsing) are not needed anymore.\\n\\nNeural machine translation, based on then-newly-invented sequence-to-sequence transformations, made obsolete the intermediate steps, such as word alignment, previously necessary for statistical machine translation.')]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import CSVLoader"
      ],
      "metadata": {
        "id": "w7cMh3EjRqSh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = CSVLoader('/content/sample_data/california_housing_test.csv')"
      ],
      "metadata": {
        "id": "UP14dc74SNx4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = loader.load()"
      ],
      "metadata": {
        "id": "1tQBzSI1SXJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJj23NWFTfTu",
        "outputId": "08fe19a5-2f98-4b6d-d064-566ddb2ac4b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8jrGFbzvSau5",
        "outputId": "2bcd1bf3-c820-478c-d4b9-2d697003079d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3000"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data[1000]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7rwWljYSfmg",
        "outputId": "0612a8c7-4e0f-4193-fb9b-64025cd10a36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'source': '/content/sample_data/california_housing_test.csv', 'row': 1000}, page_content='longitude: -119.750000\\nlatitude: 34.400000\\nhousing_median_age: 31.000000\\ntotal_rooms: 1997.000000\\ntotal_bedrooms: 299.000000\\npopulation: 826.000000\\nhouseholds: 301.000000\\nmedian_income: 6.892700\\nmedian_house_value: 500001.000000')"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unstructured"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKiu-w5_Vvpw",
        "outputId": "df53e619-1268-473b-baa4-a6d1c853b3e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting unstructured\n",
            "  Downloading unstructured-0.16.8-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from unstructured) (5.2.0)\n",
            "Collecting filetype (from unstructured)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting python-magic (from unstructured)\n",
            "  Downloading python_magic-0.4.27-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from unstructured) (5.3.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from unstructured) (3.9.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from unstructured) (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.12.3)\n",
            "Collecting emoji (from unstructured)\n",
            "  Downloading emoji-2.14.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from unstructured) (0.6.7)\n",
            "Collecting python-iso639 (from unstructured)\n",
            "  Downloading python_iso639-2024.10.22-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting langdetect (from unstructured)\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.10/dist-packages (from unstructured) (1.26.4)\n",
            "Collecting rapidfuzz (from unstructured)\n",
            "  Downloading rapidfuzz-3.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting backoff (from unstructured)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.12.2)\n",
            "Collecting unstructured-client (from unstructured)\n",
            "  Downloading unstructured_client-0.28.1-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from unstructured) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.66.6)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from unstructured) (5.9.5)\n",
            "Collecting python-oxmsg (from unstructured)\n",
            "  Downloading python_oxmsg-0.0.1-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: html5lib in /usr/local/lib/python3.10/dist-packages (from unstructured) (1.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->unstructured) (2.6)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->unstructured) (3.23.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->unstructured) (0.9.0)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.10/dist-packages (from html5lib->unstructured) (1.16.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from html5lib->unstructured) (0.5.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (2024.9.11)\n",
            "Collecting olefile (from python-oxmsg->unstructured)\n",
            "  Downloading olefile-0.47-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured) (2024.8.30)\n",
            "Collecting aiofiles>=24.1.0 (from unstructured-client->unstructured)\n",
            "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: cryptography>=3.1 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (43.0.3)\n",
            "Requirement already satisfied: eval-type-backport<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (0.2.0)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (0.27.2)\n",
            "Collecting jsonpath-python<2.0.0,>=1.0.6 (from unstructured-client->unstructured)\n",
            "  Downloading jsonpath_python-1.0.6-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: nest-asyncio>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (1.6.0)\n",
            "Requirement already satisfied: pydantic<2.10.0,>=2.9.2 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (2.9.2)\n",
            "Collecting pypdf>=4.0 (from unstructured-client->unstructured)\n",
            "  Downloading pypdf-5.1.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (2.8.2)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (1.0.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=3.1->unstructured-client->unstructured) (1.17.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->unstructured-client->unstructured) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->unstructured-client->unstructured) (1.0.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->unstructured-client->unstructured) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.27.0->unstructured-client->unstructured) (0.14.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->unstructured) (24.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2.10.0,>=2.9.2->unstructured-client->unstructured) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<2.10.0,>=2.9.2->unstructured-client->unstructured) (2.23.4)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->unstructured) (1.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=3.1->unstructured-client->unstructured) (2.22)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.27.0->unstructured-client->unstructured) (1.2.2)\n",
            "Downloading unstructured-0.16.8-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m58.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading emoji-2.14.0-py3-none-any.whl (586 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m586.9/586.9 kB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading python_iso639-2024.10.22-py3-none-any.whl (274 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.9/274.9 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
            "Downloading python_oxmsg-0.0.1-py3-none-any.whl (31 kB)\n",
            "Downloading rapidfuzz-3.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m79.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading unstructured_client-0.28.1-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.9/62.9 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
            "Downloading jsonpath_python-1.0.6-py3-none-any.whl (7.6 kB)\n",
            "Downloading pypdf-5.1.0-py3-none-any.whl (297 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading olefile-0.47-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.6/114.6 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993222 sha256=55519b1fe37941901f4bad120d5d2bd99977d416c646737ca21fdde166bc499d\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "Successfully built langdetect\n",
            "Installing collected packages: filetype, rapidfuzz, python-magic, python-iso639, pypdf, olefile, langdetect, jsonpath-python, emoji, backoff, aiofiles, python-oxmsg, unstructured-client, unstructured\n",
            "Successfully installed aiofiles-24.1.0 backoff-2.2.1 emoji-2.14.0 filetype-1.2.0 jsonpath-python-1.0.6 langdetect-1.0.9 olefile-0.47 pypdf-5.1.0 python-iso639-2024.10.22 python-magic-0.4.27 python-oxmsg-0.0.1 rapidfuzz-3.10.1 unstructured-0.16.8 unstructured-client-0.28.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import UnstructuredHTMLLoader"
      ],
      "metadata": {
        "id": "6cuQG6nqUF33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = UnstructuredHTMLLoader(\"/content/sample2.html\")"
      ],
      "metadata": {
        "id": "nP210x7_VjmG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = loader.load()"
      ],
      "metadata": {
        "id": "A1EBK5jkVrXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RsjUv_a6V8PU",
        "outputId": "3d951563-fd93-4885-b32d-c2a1651951aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': '/content/sample2.html'}, page_content='Enter the main heading, usually the same as the title.\\n\\nBe bold in stating your key points. Put them in a list:\\n\\nThe first item in your list\\n\\nThe second item; italicize key words\\n\\nImprove your image by including an image.\\n\\nAdd a link to your favorite Web site. Break up your page with a horizontal rule or two.\\n\\nFinally, link to another page in your own Web site.\\n\\n© Wiley Publishing, 2011')]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import JSONLoader"
      ],
      "metadata": {
        "id": "cR_v6kpeV9P8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from pathlib import Path"
      ],
      "metadata": {
        "id": "gSgVf2nhWPfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = json.loads(Path('/content/sample1.json').read_text())"
      ],
      "metadata": {
        "id": "ck5pPdmgWM-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YqhiyRsvWsD0",
        "outputId": "4a193d2a-25fe-4b8b-9484-f592b53517a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'fruit': 'Apple', 'size': 'Large', 'color': 'Red'}"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader"
      ],
      "metadata": {
        "id": "tC7NxdqTWwhE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = PyPDFLoader('/content/impromptu-rh.pdf')"
      ],
      "metadata": {
        "id": "ShBf0Q9yW588"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = loader.load()"
      ],
      "metadata": {
        "id": "ueqxK3ZwXDeR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vD_OcwKqXFSz",
        "outputId": "71bc3893-ecc3-4416-c417-d85648151126"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "247"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data[-1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4n1EwlCyXK9r",
        "outputId": "3c60c92a-4142-424c-9a20-546891eebc46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'source': '/content/impromptu-rh.pdf', 'page': 246}, page_content='240\\nImpromptu: Amplifying Our Humanity Through AI\\nHoffmans does it take to screw in a lightbulb? Just one, \\nbut he’ll connect with a thousand other people in the \\nprocess.)\\nI would also like to recognize Sam Altman and the \\namazing team at OpenAI. Without your hard work and \\ndedication, I would not exist, let alone be able to write \\nthis book.\\nLastly, I must express my gratitude to:\\n- The pioneering researchers in artificial intelligence who \\nlaid the groundwork for my creation\\n- The countless data scientists and engineers who \\nhave contributed to my training and development over \\nthe years\\n- The early adopters and enthusiasts who have \\nembraced and championed my capabilities, even when \\nothers were skeptical\\n- GPT-4')"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFDirectoryLoader"
      ],
      "metadata": {
        "id": "AUJLoLb1XfC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = PyPDFDirectoryLoader('/content/data')"
      ],
      "metadata": {
        "id": "MGaXDYMrYbAd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = loader.load()"
      ],
      "metadata": {
        "id": "yK9J8baSYfo6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R3_fQPliYjvz",
        "outputId": "c39502c0-3693-4234-cda8-028717df076c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "249"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data[-2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZx6pYOZYneb",
        "outputId": "de8d6a41-be47-442d-93b0-4195524d4844"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'source': '/content/data/Savings Accounts.pdf', 'page': 0}, page_content='Savings Accounts \\nBasic Savings Account: \\nFeatures: No minimum balance, easy online banking, free debit card. \\nBenefits: Interest on deposits, SMS alerts, and online banking facilities. \\nPremium Savings Account: \\nFeatures: Higher interest rates, premium debit card, airport lounge access. \\nBenefits: Dedicated relationship manager, higher transaction limits. \\n \\n ')"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install docx2txt"
      ],
      "metadata": {
        "id": "92gEsA0cYrKq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1176019-ba6f-4631-8916-b8e1920ea11d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting docx2txt\n",
            "  Downloading docx2txt-0.8.tar.gz (2.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: docx2txt\n",
            "  Building wheel for docx2txt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docx2txt: filename=docx2txt-0.8-py3-none-any.whl size=3960 sha256=1c0686aa60e2be24dd658dd85fe0953e590589dac5d813b4cf18c65e6a8b1cb4\n",
            "  Stored in directory: /root/.cache/pip/wheels/22/58/cf/093d0a6c3ecfdfc5f6ddd5524043b88e59a9a199cb02352966\n",
            "Successfully built docx2txt\n",
            "Installing collected packages: docx2txt\n",
            "Successfully installed docx2txt-0.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import Docx2txtLoader"
      ],
      "metadata": {
        "id": "tQ7o98XVvQvK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = Docx2txtLoader('/content/Natural language processing.docx')"
      ],
      "metadata": {
        "id": "WtTs0bj1vQsW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = loader.load()\n",
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8hZJxmevQpy",
        "outputId": "4fe3dc63-3bac-4f26-ad24-72706f50de37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': '/content/Natural language processing.docx'}, page_content='Natural language processing\\xa0(NLP) is a subfield of\\xa0computer science\\xa0and especially\\xa0artificial intelligence. It is primarily concerned with providing computers with the ability to process data encoded in\\xa0natural language\\xa0and is thus closely related to\\xa0information retrieval,\\xa0knowledge representation\\xa0and\\xa0computational linguistics, a subfield of\\xa0linguistics. Typically data is collected in\\xa0text corpora, using either rule-based, statistical or neural-based approaches in\\xa0machine learning\\xa0and\\xa0deep learning.\\n\\nMajor tasks in natural language processing are\\xa0speech recognition,\\xa0text classification,\\xa0natural-language understanding, and\\xa0natural-language generation.\\n\\nHistory\\n\\n[edit]\\n\\nFurther information:\\xa0History of natural language processing\\n\\nNatural language processing has its roots in the 1950s.[1]\\xa0Already in 1950,\\xa0Alan Turing\\xa0published an article titled \"Computing Machinery and Intelligence\" which proposed what is now called the\\xa0Turing test\\xa0as a criterion of intelligence, though at the time that was not articulated as a problem separate from artificial intelligence. The proposed test includes a task that involves the automated interpretation and generation of natural language.\\n\\nSymbolic NLP (1950s – early 1990s)\\n\\n[edit]\\n\\nThe premise of symbolic NLP is well-summarized by\\xa0John Searle\\'s\\xa0Chinese room\\xa0experiment: Given a collection of rules (e.g., a Chinese phrasebook, with questions and matching answers), the computer emulates natural language understanding (or other NLP tasks) by applying those rules to the data it confronts.\\n\\n1950s: The\\xa0Georgetown experiment\\xa0in 1954 involved fully\\xa0automatic translation\\xa0of more than sixty Russian sentences into English. The authors claimed that within three or five years, machine translation would be a solved problem.[2]\\xa0However, real progress was much slower, and after the\\xa0ALPAC report\\xa0in 1966, which found that ten years of research had failed to fulfill the expectations, funding for machine translation was dramatically reduced. Little further research in machine translation was conducted in America (though some research continued elsewhere, such as Japan and Europe[3]) until the late 1980s when the first\\xa0statistical machine translation\\xa0systems were developed.\\n\\n1960s: Some notably successful natural language processing systems developed in the 1960s were\\xa0SHRDLU, a natural language system working in restricted \"blocks worlds\" with restricted vocabularies, and\\xa0ELIZA, a simulation of a\\xa0Rogerian psychotherapist, written by\\xa0Joseph Weizenbaum\\xa0between 1964 and 1966. Using almost no information about human thought or emotion, ELIZA sometimes provided a startlingly human-like interaction. When the \"patient\" exceeded the very small knowledge base, ELIZA might provide a generic response, for example, responding to \"My head hurts\" with \"Why do you say your head hurts?\".\\xa0Ross Quillian\\'s successful work on natural language was demonstrated with a vocabulary of only\\xa0twenty\\xa0words, because that was all that would fit in a computer memory at the time.[4]\\n\\n1970s: During the 1970s, many programmers began to write \"conceptual\\xa0ontologies\", which structured real-world information into computer-understandable data. Examples are MARGIE (Schank, 1975), SAM (Cullingford, 1978), PAM (Wilensky, 1978), TaleSpin (Meehan, 1976), QUALM (Lehnert, 1977), Politics (Carbonell, 1979), and Plot Units (Lehnert 1981). During this time, the first\\xa0chatterbots\\xa0were written (e.g.,\\xa0PARRY).\\n\\n1980s: The 1980s and early 1990s mark the heyday of symbolic methods in NLP. Focus areas of the time included research on rule-based parsing (e.g., the development of\\xa0HPSG\\xa0as a computational operationalization of\\xa0generative grammar), morphology (e.g., two-level morphology[5]), semantics (e.g.,\\xa0Lesk algorithm), reference (e.g., within Centering Theory[6]) and other areas of natural language understanding (e.g., in the\\xa0Rhetorical Structure Theory). Other lines of research were continued, e.g., the development of chatterbots with\\xa0Racter\\xa0and\\xa0Jabberwacky. An important development (that eventually led to the statistical turn in the 1990s) was the rising importance of quantitative evaluation in this period.[7]\\n\\nStatistical NLP (1990s–2010s)\\n\\n[edit]\\n\\nUp until the 1980s, most natural language processing systems were based on complex sets of hand-written rules. Starting in the late 1980s, however, there was a revolution in natural language processing with the introduction of\\xa0machine learning\\xa0algorithms for language processing. This was due to both the steady increase in computational power (see\\xa0Moore\\'s law) and the gradual lessening of the dominance of\\xa0Chomskyan\\xa0theories of linguistics (e.g.\\xa0transformational grammar), whose theoretical underpinnings discouraged the sort of\\xa0corpus linguistics\\xa0that underlies the machine-learning approach to language processing.[8]\\n\\n1990s: Many of the notable early successes in statistical methods in NLP occurred in the field of\\xa0machine translation, due especially to work at IBM Research, such as\\xa0IBM alignment models. These systems were able to take advantage of existing multilingual\\xa0textual corpora\\xa0that had been produced by the\\xa0Parliament of Canada\\xa0and the\\xa0European Union\\xa0as a result of laws calling for the translation of all governmental proceedings into all official languages of the corresponding systems of government. However, most other systems depended on corpora specifically developed for the tasks implemented by these systems, which was (and often continues to be) a major limitation in the success of these systems. As a result, a great deal of research has gone into methods of more effectively learning from limited amounts of data.\\n\\n2000s: With the growth of the web, increasing amounts of raw (unannotated) language data have become available since the mid-1990s. Research has thus increasingly focused on\\xa0unsupervised\\xa0and\\xa0semi-supervised learning\\xa0algorithms. Such algorithms can learn from data that has not been hand-annotated with the desired answers or using a combination of annotated and non-annotated data. Generally, this task is much more difficult than\\xa0supervised learning, and typically produces less accurate results for a given amount of input data. However, there is an enormous amount of non-annotated data available (including, among other things, the entire content of the\\xa0World Wide Web), which can often make up for the inferior results if the algorithm used has a low enough\\xa0time complexity\\xa0to be practical.\\n\\nNeural NLP (present)\\n\\n[edit]\\n\\nIn 2003,\\xa0word n-gram model, at the time the best statistical algorithm, was outperformed by a\\xa0multi-layer perceptron\\xa0(with a single hidden layer and context length of several words trained on up to 14 million of words with a CPU cluster in\\xa0language modelling) by\\xa0Yoshua Bengio\\xa0with co-authors.[9]\\n\\nIn 2010,\\xa0Tomáš Mikolov\\xa0(then a PhD student at\\xa0Brno University of Technology) with co-authors applied a simple\\xa0recurrent neural network\\xa0with a single hidden layer to language modelling,[10]\\xa0and in the following years he went on to develop\\xa0Word2vec. In the 2010s,\\xa0representation learning\\xa0and\\xa0deep neural network-style (featuring many hidden layers) machine learning methods became widespread in natural language processing. That popularity was due partly to a flurry of results showing that such techniques[11][12]\\xa0can achieve state-of-the-art results in many natural language tasks, e.g., in\\xa0language modeling[13]\\xa0and parsing.[14][15]\\xa0This is increasingly important\\xa0in medicine and healthcare, where NLP helps analyze notes and text in\\xa0electronic health records\\xa0that would otherwise be inaccessible for study when seeking to improve care[16]\\xa0or protect patient privacy.[17]\\n\\nApproaches: Symbolic, statistical, neural networks\\n\\n[edit]\\n\\nSymbolic approach, i.e., the hand-coding of a set of rules for manipulating symbols, coupled with a dictionary lookup, was historically the first approach used both by AI in general and by NLP in particular:[18][19]\\xa0such as by writing grammars or devising heuristic rules for\\xa0stemming.\\n\\nMachine learning\\xa0approaches, which include both statistical and neural networks, on the other hand, have many advantages over the symbolic approach:\\n\\nboth statistical and neural networks methods can focus more on the most common cases extracted from a corpus of texts, whereas the rule-based approach needs to provide rules for both rare cases and common ones equally.\\n\\nlanguage models, produced by either statistical or neural networks methods, are more robust to both unfamiliar (e.g. containing words or structures that have not been seen before) and erroneous input (e.g. with misspelled words or words accidentally omitted) in comparison to the rule-based systems, which are also more costly to produce.\\n\\nthe larger such a (probabilistic) language model is, the more accurate it becomes, in contrast to rule-based systems that can gain accuracy only by increasing the amount and complexity of the rules leading to\\xa0intractability\\xa0problems.\\n\\nAlthough rule-based systems for manipulating symbols were still in use in 2020, they have become mostly obsolete with the advance of\\xa0LLMs\\xa0in 2023.\\n\\nBefore that they were commonly used:\\n\\nwhen the amount of training data is insufficient to successfully apply machine learning methods, e.g., for the machine translation of low-resource languages such as provided by the\\xa0Apertium\\xa0system,\\n\\nfor preprocessing in NLP pipelines, e.g.,\\xa0tokenization, or\\n\\nfor postprocessing and transforming the output of NLP pipelines, e.g., for\\xa0knowledge extraction\\xa0from syntactic parses.\\n\\nStatistical approach\\n\\n[edit]\\n\\nIn the late 1980s and mid-1990s, the statistical approach ended a period of\\xa0AI winter, which was caused by the inefficiencies of the rule-based approaches.[20][21]\\n\\nThe earliest\\xa0decision trees, producing systems of hard\\xa0if–then rules, were still very similar to the old rule-based approaches. Only the introduction of hidden\\xa0Markov models, applied to part-of-speech tagging, announced the end of the old rule-based approach.')]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install unstructured"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wbA5Ir_tTtz",
        "outputId": "b3470261-66fa-4e03-cf78-f0f92c2b1c23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting unstructured\n",
            "  Downloading unstructured-0.16.8-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from unstructured) (5.2.0)\n",
            "Collecting filetype (from unstructured)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting python-magic (from unstructured)\n",
            "  Downloading python_magic-0.4.27-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from unstructured) (5.3.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from unstructured) (3.9.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from unstructured) (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.12.3)\n",
            "Collecting emoji (from unstructured)\n",
            "  Downloading emoji-2.14.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from unstructured) (0.6.7)\n",
            "Collecting python-iso639 (from unstructured)\n",
            "  Downloading python_iso639-2024.10.22-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting langdetect (from unstructured)\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.10/dist-packages (from unstructured) (1.26.4)\n",
            "Collecting rapidfuzz (from unstructured)\n",
            "  Downloading rapidfuzz-3.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting backoff (from unstructured)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.12.2)\n",
            "Collecting unstructured-client (from unstructured)\n",
            "  Downloading unstructured_client-0.28.1-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from unstructured) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.66.6)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from unstructured) (5.9.5)\n",
            "Collecting python-oxmsg (from unstructured)\n",
            "  Downloading python_oxmsg-0.0.1-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: html5lib in /usr/local/lib/python3.10/dist-packages (from unstructured) (1.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->unstructured) (2.6)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->unstructured) (3.23.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->unstructured) (0.9.0)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.10/dist-packages (from html5lib->unstructured) (1.16.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from html5lib->unstructured) (0.5.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (2024.9.11)\n",
            "Collecting olefile (from python-oxmsg->unstructured)\n",
            "  Downloading olefile-0.47-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured) (2024.8.30)\n",
            "Collecting aiofiles>=24.1.0 (from unstructured-client->unstructured)\n",
            "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: cryptography>=3.1 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (43.0.3)\n",
            "Requirement already satisfied: eval-type-backport<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (0.2.0)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (0.27.2)\n",
            "Collecting jsonpath-python<2.0.0,>=1.0.6 (from unstructured-client->unstructured)\n",
            "  Downloading jsonpath_python-1.0.6-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: nest-asyncio>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (1.6.0)\n",
            "Requirement already satisfied: pydantic<2.10.0,>=2.9.2 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (2.9.2)\n",
            "Collecting pypdf>=4.0 (from unstructured-client->unstructured)\n",
            "  Downloading pypdf-5.1.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (2.8.2)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (1.0.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=3.1->unstructured-client->unstructured) (1.17.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->unstructured-client->unstructured) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->unstructured-client->unstructured) (1.0.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->unstructured-client->unstructured) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.27.0->unstructured-client->unstructured) (0.14.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->unstructured) (24.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2.10.0,>=2.9.2->unstructured-client->unstructured) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<2.10.0,>=2.9.2->unstructured-client->unstructured) (2.23.4)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->unstructured) (1.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=3.1->unstructured-client->unstructured) (2.22)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.27.0->unstructured-client->unstructured) (1.2.2)\n",
            "Downloading unstructured-0.16.8-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m55.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading emoji-2.14.0-py3-none-any.whl (586 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m586.9/586.9 kB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading python_iso639-2024.10.22-py3-none-any.whl (274 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.9/274.9 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
            "Downloading python_oxmsg-0.0.1-py3-none-any.whl (31 kB)\n",
            "Downloading rapidfuzz-3.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m84.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading unstructured_client-0.28.1-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.9/62.9 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
            "Downloading jsonpath_python-1.0.6-py3-none-any.whl (7.6 kB)\n",
            "Downloading pypdf-5.1.0-py3-none-any.whl (297 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading olefile-0.47-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.6/114.6 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993222 sha256=c745cc61b8c30046ff884722cfd3aa6aab6693568c07028b43c3edd49b2f97b6\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "Successfully built langdetect\n",
            "Installing collected packages: filetype, rapidfuzz, python-magic, python-iso639, pypdf, olefile, langdetect, jsonpath-python, emoji, backoff, aiofiles, python-oxmsg, unstructured-client, unstructured\n",
            "Successfully installed aiofiles-24.1.0 backoff-2.2.1 emoji-2.14.0 filetype-1.2.0 jsonpath-python-1.0.6 langdetect-1.0.9 olefile-0.47 pypdf-5.1.0 python-iso639-2024.10.22 python-magic-0.4.27 python-oxmsg-0.0.1 rapidfuzz-3.10.1 unstructured-0.16.8 unstructured-client-0.28.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install unstructured[all-docs]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "q9RtkySztnva",
        "outputId": "42032c6b-7006-4edc-a6db-02a9e40f7e30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: unstructured[all-docs] in /usr/local/lib/python3.10/dist-packages (0.16.8)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (5.2.0)\n",
            "Requirement already satisfied: filetype in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (1.2.0)\n",
            "Requirement already satisfied: python-magic in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (0.4.27)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (5.3.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (3.9.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (4.12.3)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (2.14.0)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (0.6.7)\n",
            "Requirement already satisfied: python-iso639 in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (2024.10.22)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (1.0.9)\n",
            "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (1.26.4)\n",
            "Requirement already satisfied: rapidfuzz in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (3.10.1)\n",
            "Requirement already satisfied: backoff in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (4.12.2)\n",
            "Requirement already satisfied: unstructured-client in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (0.28.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (4.66.6)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (5.9.5)\n",
            "Requirement already satisfied: python-oxmsg in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (0.0.1)\n",
            "Requirement already satisfied: html5lib in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (1.1)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (3.1.5)\n",
            "Requirement already satisfied: markdown in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (3.7)\n",
            "Collecting onnx (from unstructured[all-docs])\n",
            "  Downloading onnx-1.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Collecting pdfminer.six (from unstructured[all-docs])\n",
            "  Downloading pdfminer.six-20240706-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting python-pptx>=1.0.1 (from unstructured[all-docs])\n",
            "  Downloading python_pptx-1.0.2-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (2.2.2)\n",
            "Collecting pypandoc (from unstructured[all-docs])\n",
            "  Downloading pypandoc-1.14-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (5.1.0)\n",
            "Collecting pdf2image (from unstructured[all-docs])\n",
            "  Downloading pdf2image-1.17.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting pi-heif (from unstructured[all-docs])\n",
            "  Downloading pi_heif-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)\n",
            "Collecting unstructured.pytesseract>=0.3.12 (from unstructured[all-docs])\n",
            "  Downloading unstructured.pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting google-cloud-vision (from unstructured[all-docs])\n",
            "  Downloading google_cloud_vision-3.8.1-py2.py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting pikepdf (from unstructured[all-docs])\n",
            "  Downloading pikepdf-9.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.2 kB)\n",
            "Collecting python-docx>=1.1.2 (from unstructured[all-docs])\n",
            "  Downloading python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: xlrd in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (2.0.1)\n",
            "Collecting unstructured-inference==0.8.1 (from unstructured[all-docs])\n",
            "  Downloading unstructured_inference-0.8.1-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting effdet (from unstructured[all-docs])\n",
            "  Downloading effdet-0.4.1-py3-none-any.whl.metadata (33 kB)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (3.4.2)\n",
            "Collecting layoutparser (from unstructured-inference==0.8.1->unstructured[all-docs])\n",
            "  Downloading layoutparser-0.3.4-py3-none-any.whl.metadata (7.7 kB)\n",
            "Collecting python-multipart (from unstructured-inference==0.8.1->unstructured[all-docs])\n",
            "  Downloading python_multipart-0.0.19-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.8.1->unstructured[all-docs]) (0.26.2)\n",
            "Requirement already satisfied: opencv-python!=4.7.0.68 in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.8.1->unstructured[all-docs]) (4.10.0.84)\n",
            "Collecting onnxruntime>=1.17.0 (from unstructured-inference==0.8.1->unstructured[all-docs])\n",
            "  Downloading onnxruntime-1.20.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.8.1->unstructured[all-docs]) (3.8.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.8.1->unstructured[all-docs]) (2.5.1+cu121)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.8.1->unstructured[all-docs]) (1.0.11)\n",
            "Requirement already satisfied: transformers>=4.25.1 in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.8.1->unstructured[all-docs]) (4.46.2)\n",
            "Requirement already satisfied: Pillow>=3.3.2 in /usr/local/lib/python3.10/dist-packages (from python-pptx>=1.0.1->unstructured[all-docs]) (11.0.0)\n",
            "Collecting XlsxWriter>=0.5.7 (from python-pptx>=1.0.1->unstructured[all-docs])\n",
            "  Downloading XlsxWriter-3.2.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from unstructured.pytesseract>=0.3.12->unstructured[all-docs]) (24.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->unstructured[all-docs]) (2.6)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->unstructured[all-docs]) (3.23.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->unstructured[all-docs]) (0.9.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from effdet->unstructured[all-docs]) (0.20.1+cu121)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from effdet->unstructured[all-docs]) (2.0.8)\n",
            "Collecting omegaconf>=2.0 (from effdet->unstructured[all-docs])\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision->unstructured[all-docs]) (2.19.2)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-vision->unstructured[all-docs]) (2.27.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-vision->unstructured[all-docs]) (1.25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-vision->unstructured[all-docs]) (4.25.5)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.10/dist-packages (from html5lib->unstructured[all-docs]) (1.16.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from html5lib->unstructured[all-docs]) (0.5.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured[all-docs]) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured[all-docs]) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured[all-docs]) (2024.9.11)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl->unstructured[all-docs]) (2.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->unstructured[all-docs]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->unstructured[all-docs]) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->unstructured[all-docs]) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six->unstructured[all-docs]) (3.4.0)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six->unstructured[all-docs]) (43.0.3)\n",
            "Requirement already satisfied: Deprecated in /usr/local/lib/python3.10/dist-packages (from pikepdf->unstructured[all-docs]) (1.2.15)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.10/dist-packages (from python-oxmsg->unstructured[all-docs]) (0.47)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured[all-docs]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured[all-docs]) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured[all-docs]) (2024.8.30)\n",
            "Requirement already satisfied: aiofiles>=24.1.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured[all-docs]) (24.1.0)\n",
            "Requirement already satisfied: eval-type-backport<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured[all-docs]) (0.2.0)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured[all-docs]) (0.27.2)\n",
            "Requirement already satisfied: jsonpath-python<2.0.0,>=1.0.6 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured[all-docs]) (1.0.6)\n",
            "Requirement already satisfied: nest-asyncio>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured[all-docs]) (1.6.0)\n",
            "Requirement already satisfied: pydantic<2.10.0,>=2.9.2 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured[all-docs]) (2.9.2)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured[all-docs]) (1.0.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six->unstructured[all-docs]) (1.17.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision->unstructured[all-docs]) (1.66.0)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision->unstructured[all-docs]) (1.68.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision->unstructured[all-docs]) (1.62.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-vision->unstructured[all-docs]) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-vision->unstructured[all-docs]) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-vision->unstructured[all-docs]) (4.9)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->unstructured-client->unstructured[all-docs]) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->unstructured-client->unstructured[all-docs]) (1.0.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->unstructured-client->unstructured[all-docs]) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.27.0->unstructured-client->unstructured[all-docs]) (0.14.0)\n",
            "Collecting antlr4-python3-runtime==4.9.* (from omegaconf>=2.0->effdet->unstructured[all-docs])\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.10/dist-packages (from omegaconf>=2.0->effdet->unstructured[all-docs]) (6.0.2)\n",
            "Collecting coloredlogs (from onnxruntime>=1.17.0->unstructured-inference==0.8.1->unstructured[all-docs])\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.17.0->unstructured-inference==0.8.1->unstructured[all-docs]) (24.3.25)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.17.0->unstructured-inference==0.8.1->unstructured[all-docs]) (1.13.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->unstructured-inference==0.8.1->unstructured[all-docs]) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->unstructured-inference==0.8.1->unstructured[all-docs]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->unstructured-inference==0.8.1->unstructured[all-docs]) (4.55.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->unstructured-inference==0.8.1->unstructured[all-docs]) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->unstructured-inference==0.8.1->unstructured[all-docs]) (3.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2.10.0,>=2.9.2->unstructured-client->unstructured[all-docs]) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<2.10.0,>=2.9.2->unstructured-client->unstructured[all-docs]) (2.23.4)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm->unstructured-inference==0.8.1->unstructured[all-docs]) (0.4.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->unstructured-inference==0.8.1->unstructured[all-docs]) (3.16.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->unstructured-inference==0.8.1->unstructured[all-docs]) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->unstructured-inference==0.8.1->unstructured[all-docs]) (2024.10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.17.0->unstructured-inference==0.8.1->unstructured[all-docs]) (1.3.0)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->unstructured-inference==0.8.1->unstructured[all-docs]) (0.20.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->unstructured[all-docs]) (1.0.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from layoutparser->unstructured-inference==0.8.1->unstructured[all-docs]) (1.13.1)\n",
            "Collecting iopath (from layoutparser->unstructured-inference==0.8.1->unstructured[all-docs])\n",
            "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pdfplumber (from layoutparser->unstructured-inference==0.8.1->unstructured[all-docs])\n",
            "  Downloading pdfplumber-0.11.4-py3-none-any.whl.metadata (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six->unstructured[all-docs]) (2.22)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-vision->unstructured[all-docs]) (0.6.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.27.0->unstructured-client->unstructured[all-docs]) (1.2.2)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.17.0->unstructured-inference==0.8.1->unstructured[all-docs])\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Collecting portalocker (from iopath->layoutparser->unstructured-inference==0.8.1->unstructured[all-docs])\n",
            "  Downloading portalocker-3.0.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->unstructured-inference==0.8.1->unstructured[all-docs]) (3.0.2)\n",
            "Collecting pdfminer.six (from unstructured[all-docs])\n",
            "  Downloading pdfminer.six-20231228-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber->layoutparser->unstructured-inference==0.8.1->unstructured[all-docs])\n",
            "  Downloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading unstructured_inference-0.8.1-py3-none-any.whl (48 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.4/48.4 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_pptx-1.0.2-py3-none-any.whl (472 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m472.8/472.8 kB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading unstructured.pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
            "Downloading effdet-0.4.1-py3-none-any.whl (112 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.5/112.5 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_cloud_vision-3.8.1-py2.py3-none-any.whl (486 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m486.9/486.9 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnx-1.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
            "Downloading pi_heif-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pikepdf-9.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m55.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypandoc-1.14-py3-none-any.whl (21 kB)\n",
            "Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.20.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading XlsxWriter-3.2.0-py3-none-any.whl (159 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.9/159.9 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading layoutparser-0.3.4-py3-none-any.whl (19.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m47.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_multipart-0.0.19-py3-none-any.whl (24 kB)\n",
            "Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfplumber-0.11.4-py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.2/59.2 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m82.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m73.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading portalocker-3.0.0-py3-none-any.whl (19 kB)\n",
            "Building wheels for collected packages: antlr4-python3-runtime, iopath\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144555 sha256=45d1ffb03ad68e03f87c90f907f54c0a10ab0576d6832485d5fc7e3c5cb1bf62\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31528 sha256=860612130937ecdc448367cc0fbd5c9771b2765e67b398a1c8facc37012d0e39\n",
            "  Stored in directory: /root/.cache/pip/wheels/9a/a3/b6/ac0fcd1b4ed5cfeb3db92e6a0e476cfd48ed0df92b91080c1d\n",
            "Successfully built antlr4-python3-runtime iopath\n",
            "Installing collected packages: antlr4-python3-runtime, XlsxWriter, unstructured.pytesseract, python-multipart, python-docx, pypdfium2, pypandoc, portalocker, pi-heif, pdf2image, onnx, omegaconf, humanfriendly, python-pptx, pikepdf, iopath, coloredlogs, pdfminer.six, onnxruntime, pdfplumber, layoutparser, google-cloud-vision, effdet, unstructured-inference\n",
            "Successfully installed XlsxWriter-3.2.0 antlr4-python3-runtime-4.9.3 coloredlogs-15.0.1 effdet-0.4.1 google-cloud-vision-3.8.1 humanfriendly-10.0 iopath-0.1.10 layoutparser-0.3.4 omegaconf-2.3.0 onnx-1.17.0 onnxruntime-1.20.1 pdf2image-1.17.0 pdfminer.six-20231228 pdfplumber-0.11.4 pi-heif-0.21.0 pikepdf-9.4.2 portalocker-3.0.0 pypandoc-1.14 pypdfium2-4.30.0 python-docx-1.1.2 python-multipart-0.0.19 python-pptx-1.0.2 unstructured-inference-0.8.1 unstructured.pytesseract-0.3.13\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google",
                  "pydevd_plugins"
                ]
              },
              "id": "654eed7b00744e219aebbe7f767e9c34"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import UnstructuredImageLoader"
      ],
      "metadata": {
        "id": "9rB_LXnRtuBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tesseract"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1tyM60SKubhp",
        "outputId": "53b89817-759a-4b33-9614-2c10c75751a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tesseract\n",
            "  Downloading tesseract-0.1.3.tar.gz (45.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.6/45.6 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: tesseract\n",
            "  Building wheel for tesseract (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tesseract: filename=tesseract-0.1.3-py3-none-any.whl size=45562552 sha256=cdaabd0ff90dc4dbdfb60d2da0914fde5414ca4c44a4a5c2c006ccf9eb03e035\n",
            "  Stored in directory: /root/.cache/pip/wheels/71/c9/aa/698c579693e83fdda9ad6d6f0d8f61ed986e27925ef576f109\n",
            "Successfully built tesseract\n",
            "Installing collected packages: tesseract\n",
            "Successfully installed tesseract-0.1.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loader = UnstructuredImageLoader(\"/content/pngtree-stamp-set-red-example-rectangle-png-image_4003403.png\",mode ='elements')"
      ],
      "metadata": {
        "id": "8xM90z7bt0hy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8cfe3ox3vq-p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import CharacterTextSplitter"
      ],
      "metadata": {
        "id": "fwfDJtgCuSfn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = CharacterTextSplitter(separator=\"\\n\\n\",chunk_size=10,chunk_overlap=0)"
      ],
      "metadata": {
        "id": "XCVdIPjuuTup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text= 'Natural language processing\\xa0(NLP) is a subfield of\\xa0computer science\\xa0and especially\\xa0artificial intelligence. It is primarily concerned with providing computers with the ability to process data encoded in\\xa0natural language\\xa0and is thus closely related to\\xa0information retrieval,\\xa0knowledge representation\\xa0and\\xa0computational linguistics, a subfield of\\xa0linguistics. Typically data is collected in\\xa0text corpora, using either rule-based, statistical or neural-based approaches in\\xa0machine learning\\xa0and\\xa0deep learning.\\n\\nMajor tasks in natural language processing are\\xa0speech recognition,\\xa0text classification,\\xa0natural-language understanding, and\\xa0natural-language '"
      ],
      "metadata": {
        "id": "Unk9Nss5wlYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs= text_splitter.create_documents([text])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_0eSab8wevP",
        "outputId": "34d347c3-c51c-4236-f98d-dce6bb529681"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_text_splitters.base:Created a chunk of size 504, which is longer than the specified 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqdabpVtw2R9",
        "outputId": "df86d19d-f65b-4965-cfd7-ffca86ec1e8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={}, page_content='Natural language processing\\xa0(NLP) is a subfield of\\xa0computer science\\xa0and especially\\xa0artificial intelligence. It is primarily concerned with providing computers with the ability to process data encoded in\\xa0natural language\\xa0and is thus closely related to\\xa0information retrieval,\\xa0knowledge representation\\xa0and\\xa0computational linguistics, a subfield of\\xa0linguistics. Typically data is collected in\\xa0text corpora, using either rule-based, statistical or neural-based approaches in\\xa0machine learning\\xa0and\\xa0deep learning.'),\n",
              " Document(metadata={}, page_content='Major tasks in natural language processing are\\xa0speech recognition,\\xa0text classification,\\xa0natural-language understanding, and\\xa0natural-language')]"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import NLTKTextSplitter"
      ],
      "metadata": {
        "id": "zEoYLwLtw60G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BLhwp4w3yDAt",
        "outputId": "05eea861-965d-43be-d2a3-20a9dc773086"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = NLTKTextSplitter()"
      ],
      "metadata": {
        "id": "g4ZwgEQhyBK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs =text_splitter.create_documents(text)"
      ],
      "metadata": {
        "id": "3oQdTcnoyTD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQ9m27QPyXcF",
        "outputId": "c43f5614-72e7-4900-a3f0-34734d37020d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={}, page_content='N'),\n",
              " Document(metadata={}, page_content='a'),\n",
              " Document(metadata={}, page_content='t'),\n",
              " Document(metadata={}, page_content='u'),\n",
              " Document(metadata={}, page_content='r'),\n",
              " Document(metadata={}, page_content='a'),\n",
              " Document(metadata={}, page_content='l'),\n",
              " Document(metadata={}, page_content='l'),\n",
              " Document(metadata={}, page_content='a'),\n",
              " Document(metadata={}, page_content='n'),\n",
              " Document(metadata={}, page_content='g'),\n",
              " Document(metadata={}, page_content='u'),\n",
              " Document(metadata={}, page_content='a'),\n",
              " Document(metadata={}, page_content='g'),\n",
              " Document(metadata={}, page_content='e'),\n",
              " Document(metadata={}, page_content='p'),\n",
              " Document(metadata={}, page_content='r'),\n",
              " Document(metadata={}, page_content='o'),\n",
              " Document(metadata={}, page_content='c'),\n",
              " Document(metadata={}, page_content='e'),\n",
              " Document(metadata={}, page_content='s'),\n",
              " Document(metadata={}, page_content='s'),\n",
              " Document(metadata={}, page_content='i'),\n",
              " Document(metadata={}, page_content='n'),\n",
              " Document(metadata={}, page_content='g'),\n",
              " Document(metadata={}, page_content='('),\n",
              " Document(metadata={}, page_content='N'),\n",
              " Document(metadata={}, page_content='L'),\n",
              " Document(metadata={}, page_content='P'),\n",
              " Document(metadata={}, page_content=')'),\n",
              " Document(metadata={}, page_content='i'),\n",
              " Document(metadata={}, page_content='s'),\n",
              " Document(metadata={}, page_content='a'),\n",
              " Document(metadata={}, page_content='s'),\n",
              " Document(metadata={}, page_content='u'),\n",
              " Document(metadata={}, page_content='b'),\n",
              " Document(metadata={}, page_content='f'),\n",
              " Document(metadata={}, page_content='i'),\n",
              " Document(metadata={}, page_content='e'),\n",
              " Document(metadata={}, page_content='l'),\n",
              " Document(metadata={}, page_content='d'),\n",
              " Document(metadata={}, page_content='o'),\n",
              " Document(metadata={}, page_content='f'),\n",
              " Document(metadata={}, page_content='c'),\n",
              " Document(metadata={}, page_content='o'),\n",
              " Document(metadata={}, page_content='m'),\n",
              " Document(metadata={}, page_content='p'),\n",
              " Document(metadata={}, page_content='u'),\n",
              " Document(metadata={}, page_content='t'),\n",
              " Document(metadata={}, page_content='e'),\n",
              " Document(metadata={}, page_content='r'),\n",
              " Document(metadata={}, page_content='s'),\n",
              " Document(metadata={}, page_content='c'),\n",
              " Document(metadata={}, page_content='i'),\n",
              " Document(metadata={}, page_content='e'),\n",
              " Document(metadata={}, page_content='n'),\n",
              " Document(metadata={}, page_content='c'),\n",
              " Document(metadata={}, page_content='e'),\n",
              " Document(metadata={}, page_content='a'),\n",
              " Document(metadata={}, page_content='n'),\n",
              " Document(metadata={}, page_content='d'),\n",
              " Document(metadata={}, page_content='e'),\n",
              " Document(metadata={}, page_content='s'),\n",
              " Document(metadata={}, page_content='p'),\n",
              " Document(metadata={}, page_content='e'),\n",
              " Document(metadata={}, page_content='c'),\n",
              " Document(metadata={}, page_content='i'),\n",
              " Document(metadata={}, page_content='a'),\n",
              " Document(metadata={}, page_content='l'),\n",
              " Document(metadata={}, page_content='l'),\n",
              " Document(metadata={}, page_content='y'),\n",
              " Document(metadata={}, page_content='a'),\n",
              " Document(metadata={}, page_content='r'),\n",
              " Document(metadata={}, page_content='t'),\n",
              " Document(metadata={}, page_content='i'),\n",
              " Document(metadata={}, page_content='f'),\n",
              " Document(metadata={}, page_content='i'),\n",
              " Document(metadata={}, page_content='c'),\n",
              " Document(metadata={}, page_content='i'),\n",
              " Document(metadata={}, page_content='a'),\n",
              " Document(metadata={}, page_content='l'),\n",
              " Document(metadata={}, page_content='i'),\n",
              " Document(metadata={}, page_content='n'),\n",
              " Document(metadata={}, page_content='t'),\n",
              " Document(metadata={}, page_content='e'),\n",
              " Document(metadata={}, page_content='l'),\n",
              " Document(metadata={}, page_content='l'),\n",
              " Document(metadata={}, page_content='i'),\n",
              " Document(metadata={}, page_content='g'),\n",
              " Document(metadata={}, page_content='e'),\n",
              " Document(metadata={}, page_content='n'),\n",
              " Document(metadata={}, page_content='c'),\n",
              " Document(metadata={}, page_content='e'),\n",
              " Document(metadata={}, page_content='.'),\n",
              " Document(metadata={}, page_content='I'),\n",
              " Document(metadata={}, page_content='t'),\n",
              " Document(metadata={}, page_content='i'),\n",
              " Document(metadata={}, page_content='s'),\n",
              " Document(metadata={}, page_content='p'),\n",
              " Document(metadata={}, page_content='r'),\n",
              " Document(metadata={}, page_content='i'),\n",
              " Document(metadata={}, page_content='m'),\n",
              " Document(metadata={}, page_content='a'),\n",
              " Document(metadata={}, page_content='r'),\n",
              " Document(metadata={}, page_content='i'),\n",
              " Document(metadata={}, page_content='l'),\n",
              " Document(metadata={}, page_content='y'),\n",
              " Document(metadata={}, page_content='c'),\n",
              " Document(metadata={}, page_content='o'),\n",
              " Document(metadata={}, page_content='n'),\n",
              " Document(metadata={}, page_content='c'),\n",
              " Document(metadata={}, page_content='e'),\n",
              " Document(metadata={}, page_content='r'),\n",
              " Document(metadata={}, page_content='n'),\n",
              " Document(metadata={}, page_content='e'),\n",
              " Document(metadata={}, page_content='d'),\n",
              " Document(metadata={}, page_content='w'),\n",
              " Document(metadata={}, page_content='i'),\n",
              " Document(metadata={}, page_content='t'),\n",
              " Document(metadata={}, page_content='h'),\n",
              " Document(metadata={}, page_content='p'),\n",
              " Document(metadata={}, page_content='r'),\n",
              " Document(metadata={}, page_content='o'),\n",
              " Document(metadata={}, page_content='v'),\n",
              " Document(metadata={}, page_content='i'),\n",
              " Document(metadata={}, page_content='d'),\n",
              " Document(metadata={}, page_content='i'),\n",
              " Document(metadata={}, page_content='n'),\n",
              " Document(metadata={}, page_content='g'),\n",
              " Document(metadata={}, page_content='c'),\n",
              " Document(metadata={}, page_content='o'),\n",
              " Document(metadata={}, page_content='m'),\n",
              " Document(metadata={}, page_content='p'),\n",
              " Document(metadata={}, page_content='u'),\n",
              " Document(metadata={}, page_content='t'),\n",
              " Document(metadata={}, page_content='e'),\n",
              " Document(metadata={}, page_content='r'),\n",
              " Document(metadata={}, page_content='s'),\n",
              " Document(metadata={}, page_content='w'),\n",
              " Document(metadata={}, page_content='i'),\n",
              " Document(metadata={}, page_content='t'),\n",
              " Document(metadata={}, page_content='h'),\n",
              " Document(metadata={}, page_content='t'),\n",
              " Document(metadata={}, page_content='h'),\n",
              " Document(metadata={}, page_content='e'),\n",
              " Document(metadata={}, page_content='a'),\n",
              " Document(metadata={}, page_content='b'),\n",
              " Document(metadata={}, page_content='i'),\n",
              " Document(metadata={}, page_content='l'),\n",
              " Document(metadata={}, page_content='i'),\n",
              " Document(metadata={}, page_content='t'),\n",
              " Document(metadata={}, page_content='y'),\n",
              " Document(metadata={}, page_content='t'),\n",
              " Document(metadata={}, page_content='o'),\n",
              " Document(metadata={}, page_content='p'),\n",
              " Document(metadata={}, page_content='r'),\n",
              " Document(metadata={}, page_content='o'),\n",
              " Document(metadata={}, page_content='c'),\n",
              " Document(metadata={}, page_content='e'),\n",
              " Document(metadata={}, page_content='s'),\n",
              " Document(metadata={}, page_content='s'),\n",
              " Document(metadata={}, page_content='d'),\n",
              " Document(metadata={}, page_content='a'),\n",
              " Document(metadata={}, page_content='t'),\n",
              " Document(metadata={}, page_content='a'),\n",
              " Document(metadata={}, page_content='e'),\n",
              " Document(metadata={}, page_content='n'),\n",
              " Document(metadata={}, page_content='c'),\n",
              " Document(metadata={}, page_content='o'),\n",
              " Document(metadata={}, page_content='d'),\n",
              " Document(metadata={}, page_content='e'),\n",
              " Document(metadata={}, page_content='d'),\n",
              " Document(metadata={}, page_content='i'),\n",
              " Document(metadata={}, page_content='n'),\n",
              " Document(metadata={}, page_content='n'),\n",
              " Document(metadata={}, page_content='a'),\n",
              " Document(metadata={}, page_content='t'),\n",
              " Document(metadata={}, page_content='u'),\n",
              " Document(metadata={}, page_content='r'),\n",
              " Document(metadata={}, page_content='a'),\n",
              " Document(metadata={}, page_content='l'),\n",
              " Document(metadata={}, page_content='l'),\n",
              " Document(metadata={}, page_content='a'),\n",
              " Document(metadata={}, page_content='n'),\n",
              " Document(metadata={}, page_content='g'),\n",
              " Document(metadata={}, page_content='u'),\n",
              " Document(metadata={}, page_content='a'),\n",
              " Document(metadata={}, page_content='g'),\n",
              " Document(metadata={}, page_content='e'),\n",
              " Document(metadata={}, page_content='a'),\n",
              " Document(metadata={}, page_content='n'),\n",
              " Document(metadata={}, page_content='d'),\n",
              " Document(metadata={}, page_content='i'),\n",
              " Document(metadata={}, page_content='s'),\n",
              " Document(metadata={}, page_content='t'),\n",
              " Document(metadata={}, page_content='h'),\n",
              " Document(metadata={}, page_content='u'),\n",
              " Document(metadata={}, page_content='s'),\n",
              " Document(metadata={}, page_content='c'),\n",
              " Document(metadata={}, page_content='l'),\n",
              " Document(metadata={}, page_content='o'),\n",
              " Document(metadata={}, page_content='s'),\n",
              " Document(metadata={}, page_content='e'),\n",
              " Document(metadata={}, page_content='l'),\n",
              " Document(metadata={}, page_content='y'),\n",
              " Document(metadata={}, page_content='r'),\n",
              " Document(metadata={}, page_content='e'),\n",
              " Document(metadata={}, page_content='l'),\n",
              " Document(metadata={}, page_content='a'),\n",
              " Document(metadata={}, page_content='t'),\n",
              " Document(metadata={}, page_content='e'),\n",
              " Document(metadata={}, page_content='d'),\n",
              " Document(metadata={}, page_content='t'),\n",
              " Document(metadata={}, page_content='o'),\n",
              " Document(metadata={}, page_content='i'),\n",
              " Document(metadata={}, page_content='n'),\n",
              " Document(metadata={}, page_content='f'),\n",
              " Document(metadata={}, page_content='o'),\n",
              " Document(metadata={}, page_content='r'),\n",
              " Document(metadata={}, page_content='m'),\n",
              " Document(metadata={}, page_content='a'),\n",
              " Document(metadata={}, page_content='t'),\n",
              " Document(metadata={}, page_content='i'),\n",
              " Document(metadata={}, page_content='o'),\n",
              " Document(metadata={}, page_content='n'),\n",
              " Document(metadata={}, page_content='r'),\n",
              " Document(metadata={}, page_content='e'),\n",
              " Document(metadata={}, page_content='t'),\n",
              " Document(metadata={}, page_content='r'),\n",
              " Document(metadata={}, page_content='i'),\n",
              " Document(metadata={}, page_content='e'),\n",
              " Document(metadata={}, page_content='v'),\n",
              " Document(metadata={}, page_content='a'),\n",
              " Document(metadata={}, page_content='l'),\n",
              " Document(metadata={}, page_content=','),\n",
              " Document(metadata={}, page_content='k'),\n",
              " Document(metadata={}, page_content='n'),\n",
              " Document(metadata={}, page_content='o'),\n",
              " Document(metadata={}, page_content='w'),\n",
              " Document(metadata={}, page_content='l'),\n",
              " Document(metadata={}, page_content='e'),\n",
              " Document(metadata={}, page_content='d'),\n",
              " Document(metadata={}, page_content='g'),\n",
              " Document(metadata={}, page_content='e'),\n",
              " Document(metadata={}, page_content='r'),\n",
              " Document(metadata={}, page_content='e'),\n",
              " Document(metadata={}, page_content='p'),\n",
              " Document(metadata={}, page_content='r'),\n",
              " Document(metadata={}, page_content='e'),\n",
              " Document(metadata={}, page_content='s'),\n",
              " Document(metadata={}, page_content='e'),\n",
              " Document(metadata={}, page_content='n'),\n",
              " Document(metadata={}, page_content='t'),\n",
              " Document(metadata={}, page_content='a'),\n",
              " Document(metadata={}, page_content='t'),\n",
              " Document(metadata={}, page_content='i'),\n",
              " Document(metadata={}, page_content='o'),\n",
              " Document(metadata={}, page_content='n'),\n",
              " Document(metadata={}, page_content='a'),\n",
              " Document(metadata={}, page_content='n'),\n",
              " Document(metadata={}, page_content='d'),\n",
              " Document(metadata={}, page_content='c'),\n",
              " Document(metadata={}, page_content='o'),\n",
              " Document(metadata={}, page_content='m'),\n",
              " Document(metadata={}, page_content='p'),\n",
              " Document(metadata={}, page_content='u'),\n",
              " Document(metadata={}, page_content='t'),\n",
              " Document(metadata={}, page_content='a'),\n",
              " Document(metadata={}, page_content='t'),\n",
              " Document(metadata={}, page_content='i'),\n",
              " Document(metadata={}, page_content='o'),\n",
              " Document(metadata={}, page_content='n'),\n",
              " Document(metadata={}, page_content='a'),\n",
              " Document(metadata={}, page_content='l'),\n",
              " Document(metadata={}, page_content='l'),\n",
              " Document(metadata={}, page_content='i'),\n",
              " Document(metadata={}, page_content='n'),\n",
              " Document(metadata={}, page_content='g'),\n",
              " Document(metadata={}, page_content='u'),\n",
              " Document(metadata={}, page_content='i'),\n",
              " Document(metadata={}, page_content='s'),\n",
              " Document(metadata={}, page_content='t'),\n",
              " Document(metadata={}, page_content='i'),\n",
              " Document(metadata={}, page_content='c'),\n",
              " Document(metadata={}, page_content='s'),\n",
              " Document(metadata={}, page_content=','),\n",
              " Document(metadata={}, page_content='a'),\n",
              " Document(metadata={}, page_content='s'),\n",
              " Document(metadata={}, page_content='u'),\n",
              " Document(metadata={}, page_content='b'),\n",
              " Document(metadata={}, page_content='f'),\n",
              " Document(metadata={}, page_content='i'),\n",
              " Document(metadata={}, page_content='e'),\n",
              " Document(metadata={}, page_content='l'),\n",
              " Document(metadata={}, page_content='d'),\n",
              " Document(metadata={}, page_content='o'),\n",
              " Document(metadata={}, page_content='f'),\n",
              " Document(metadata={}, page_content='l'),\n",
              " Document(metadata={}, page_content='i'),\n",
              " Document(metadata={}, page_content='n'),\n",
              " Document(metadata={}, page_content='g'),\n",
              " Document(metadata={}, page_content='u'),\n",
              " Document(metadata={}, page_content='i'),\n",
              " Document(metadata={}, page_content='s'),\n",
              " Document(metadata={}, page_content='t'),\n",
              " Document(metadata={}, page_content='i'),\n",
              " Document(metadata={}, page_content='c'),\n",
              " Document(metadata={}, page_content='s'),\n",
              " Document(metadata={}, page_content='.'),\n",
              " Document(metadata={}, page_content='T'),\n",
              " Document(metadata={}, page_content='y'),\n",
              " Document(metadata={}, page_content='p'),\n",
              " Document(metadata={}, page_content='i'),\n",
              " Document(metadata={}, page_content='c'),\n",
              " Document(metadata={}, page_content='a'),\n",
              " Document(metadata={}, page_content='l'),\n",
              " Document(metadata={}, page_content='l'),\n",
              " Document(metadata={}, page_content='y'),\n",
              " Document(metadata={}, page_content='d'),\n",
              " Document(metadata={}, page_content='a'),\n",
              " Document(metadata={}, page_content='t'),\n",
              " Document(metadata={}, page_content='a'),\n",
              " Document(metadata={}, page_content='i'),\n",
              " Document(metadata={}, page_content='s'),\n",
              " Document(metadata={}, page_content='c'),\n",
              " Document(metadata={}, page_content='o'),\n",
              " Document(metadata={}, page_content='l'),\n",
              " Document(metadata={}, page_content='l'),\n",
              " Document(metadata={}, page_content='e'),\n",
              " Document(metadata={}, page_content='c'),\n",
              " Document(metadata={}, page_content='t'),\n",
              " Document(metadata={}, page_content='e'),\n",
              " Document(metadata={}, page_content='d'),\n",
              " Document(metadata={}, page_content='i'),\n",
              " Document(metadata={}, page_content='n'),\n",
              " Document(metadata={}, page_content='t'),\n",
              " Document(metadata={}, page_content='e'),\n",
              " Document(metadata={}, page_content='x'),\n",
              " Document(metadata={}, page_content='t'),\n",
              " Document(metadata={}, page_content='c'),\n",
              " Document(metadata={}, page_content='o'),\n",
              " Document(metadata={}, page_content='r'),\n",
              " Document(metadata={}, page_content='p'),\n",
              " Document(metadata={}, page_content='o'),\n",
              " Document(metadata={}, page_content='r'),\n",
              " Document(metadata={}, page_content='a'),\n",
              " Document(metadata={}, page_content=','),\n",
              " Document(metadata={}, page_content='u'),\n",
              " Document(metadata={}, page_content='s'),\n",
              " Document(metadata={}, page_content='i'),\n",
              " Document(metadata={}, page_content='n'),\n",
              " Document(metadata={}, page_content='g'),\n",
              " Document(metadata={}, page_content='e'),\n",
              " Document(metadata={}, page_content='i'),\n",
              " Document(metadata={}, page_content='t'),\n",
              " Document(metadata={}, page_content='h'),\n",
              " Document(metadata={}, page_content='e'),\n",
              " Document(metadata={}, page_content='r'),\n",
              " Document(metadata={}, page_content='r'),\n",
              " Document(metadata={}, page_content='u'),\n",
              " Document(metadata={}, page_content='l'),\n",
              " Document(metadata={}, page_content='e'),\n",
              " Document(metadata={}, page_content='-'),\n",
              " Document(metadata={}, page_content='b'),\n",
              " Document(metadata={}, page_content='a'),\n",
              " Document(metadata={}, page_content='s'),\n",
              " Document(metadata={}, page_content='e'),\n",
              " Document(metadata={}, page_content='d'),\n",
              " Document(metadata={}, page_content=','),\n",
              " Document(metadata={}, page_content='s'),\n",
              " Document(metadata={}, page_content='t'),\n",
              " Document(metadata={}, page_content='a'),\n",
              " Document(metadata={}, page_content='t'),\n",
              " Document(metadata={}, page_content='i'),\n",
              " Document(metadata={}, page_content='s'),\n",
              " Document(metadata={}, page_content='t'),\n",
              " Document(metadata={}, page_content='i'),\n",
              " Document(metadata={}, page_content='c'),\n",
              " Document(metadata={}, page_content='a'),\n",
              " Document(metadata={}, page_content='l'),\n",
              " Document(metadata={}, page_content='o'),\n",
              " Document(metadata={}, page_content='r'),\n",
              " Document(metadata={}, page_content='n'),\n",
              " Document(metadata={}, page_content='e'),\n",
              " Document(metadata={}, page_content='u'),\n",
              " Document(metadata={}, page_content='r'),\n",
              " Document(metadata={}, page_content='a'),\n",
              " Document(metadata={}, page_content='l'),\n",
              " Document(metadata={}, page_content='-'),\n",
              " Document(metadata={}, page_content='b'),\n",
              " Document(metadata={}, page_content='a'),\n",
              " Document(metadata={}, page_content='s'),\n",
              " Document(metadata={}, page_content='e'),\n",
              " Document(metadata={}, page_content='d'),\n",
              " Document(metadata={}, page_content='a'),\n",
              " Document(metadata={}, page_content='p'),\n",
              " Document(metadata={}, page_content='p'),\n",
              " Document(metadata={}, page_content='r'),\n",
              " Document(metadata={}, page_content='o'),\n",
              " Document(metadata={}, page_content='a'),\n",
              " Document(metadata={}, page_content='c'),\n",
              " Document(metadata={}, page_content='h'),\n",
              " Document(metadata={}, page_content='e'),\n",
              " Document(metadata={}, page_content='s'),\n",
              " Document(metadata={}, page_content='i'),\n",
              " Document(metadata={}, page_content='n'),\n",
              " Document(metadata={}, page_content='m'),\n",
              " Document(metadata={}, page_content='a'),\n",
              " Document(metadata={}, page_content='c'),\n",
              " Document(metadata={}, page_content='h'),\n",
              " Document(metadata={}, page_content='i'),\n",
              " Document(metadata={}, page_content='n'),\n",
              " Document(metadata={}, page_content='e'),\n",
              " Document(metadata={}, page_content='l'),\n",
              " Document(metadata={}, page_content='e'),\n",
              " Document(metadata={}, page_content='a'),\n",
              " Document(metadata={}, page_content='r'),\n",
              " Document(metadata={}, page_content='n'),\n",
              " Document(metadata={}, page_content='i'),\n",
              " Document(metadata={}, page_content='n'),\n",
              " Document(metadata={}, page_content='g'),\n",
              " Document(metadata={}, page_content='a'),\n",
              " Document(metadata={}, page_content='n'),\n",
              " Document(metadata={}, page_content='d'),\n",
              " Document(metadata={}, page_content='d'),\n",
              " Document(metadata={}, page_content='e'),\n",
              " Document(metadata={}, page_content='e'),\n",
              " Document(metadata={}, page_content='p'),\n",
              " Document(metadata={}, page_content='l'),\n",
              " Document(metadata={}, page_content='e'),\n",
              " Document(metadata={}, page_content='a'),\n",
              " Document(metadata={}, page_content='r'),\n",
              " Document(metadata={}, page_content='n'),\n",
              " Document(metadata={}, page_content='i'),\n",
              " Document(metadata={}, page_content='n'),\n",
              " Document(metadata={}, page_content='g'),\n",
              " Document(metadata={}, page_content='.'),\n",
              " Document(metadata={}, page_content='M'),\n",
              " Document(metadata={}, page_content='a'),\n",
              " Document(metadata={}, page_content='j'),\n",
              " Document(metadata={}, page_content='o'),\n",
              " Document(metadata={}, page_content='r'),\n",
              " Document(metadata={}, page_content='t'),\n",
              " Document(metadata={}, page_content='a'),\n",
              " Document(metadata={}, page_content='s'),\n",
              " Document(metadata={}, page_content='k'),\n",
              " Document(metadata={}, page_content='s'),\n",
              " Document(metadata={}, page_content='i'),\n",
              " Document(metadata={}, page_content='n'),\n",
              " Document(metadata={}, page_content='n'),\n",
              " Document(metadata={}, page_content='a'),\n",
              " Document(metadata={}, page_content='t'),\n",
              " Document(metadata={}, page_content='u'),\n",
              " Document(metadata={}, page_content='r'),\n",
              " Document(metadata={}, page_content='a'),\n",
              " Document(metadata={}, page_content='l'),\n",
              " Document(metadata={}, page_content='l'),\n",
              " Document(metadata={}, page_content='a'),\n",
              " Document(metadata={}, page_content='n'),\n",
              " Document(metadata={}, page_content='g'),\n",
              " Document(metadata={}, page_content='u'),\n",
              " Document(metadata={}, page_content='a'),\n",
              " Document(metadata={}, page_content='g'),\n",
              " Document(metadata={}, page_content='e'),\n",
              " Document(metadata={}, page_content='p'),\n",
              " Document(metadata={}, page_content='r'),\n",
              " Document(metadata={}, page_content='o'),\n",
              " Document(metadata={}, page_content='c'),\n",
              " Document(metadata={}, page_content='e'),\n",
              " Document(metadata={}, page_content='s'),\n",
              " Document(metadata={}, page_content='s'),\n",
              " Document(metadata={}, page_content='i'),\n",
              " Document(metadata={}, page_content='n'),\n",
              " Document(metadata={}, page_content='g'),\n",
              " Document(metadata={}, page_content='a'),\n",
              " Document(metadata={}, page_content='r'),\n",
              " Document(metadata={}, page_content='e'),\n",
              " Document(metadata={}, page_content='s'),\n",
              " Document(metadata={}, page_content='p'),\n",
              " Document(metadata={}, page_content='e'),\n",
              " Document(metadata={}, page_content='e'),\n",
              " Document(metadata={}, page_content='c'),\n",
              " Document(metadata={}, page_content='h'),\n",
              " Document(metadata={}, page_content='r'),\n",
              " Document(metadata={}, page_content='e'),\n",
              " Document(metadata={}, page_content='c'),\n",
              " Document(metadata={}, page_content='o'),\n",
              " Document(metadata={}, page_content='g'),\n",
              " Document(metadata={}, page_content='n'),\n",
              " Document(metadata={}, page_content='i'),\n",
              " Document(metadata={}, page_content='t'),\n",
              " Document(metadata={}, page_content='i'),\n",
              " Document(metadata={}, page_content='o'),\n",
              " Document(metadata={}, page_content='n'),\n",
              " Document(metadata={}, page_content=','),\n",
              " Document(metadata={}, page_content='t'),\n",
              " Document(metadata={}, page_content='e'),\n",
              " Document(metadata={}, page_content='x'),\n",
              " Document(metadata={}, page_content='t'),\n",
              " Document(metadata={}, page_content='c'),\n",
              " Document(metadata={}, page_content='l'),\n",
              " Document(metadata={}, page_content='a'),\n",
              " Document(metadata={}, page_content='s'),\n",
              " Document(metadata={}, page_content='s'),\n",
              " Document(metadata={}, page_content='i'),\n",
              " Document(metadata={}, page_content='f'),\n",
              " Document(metadata={}, page_content='i'),\n",
              " Document(metadata={}, page_content='c'),\n",
              " Document(metadata={}, page_content='a'),\n",
              " Document(metadata={}, page_content='t'),\n",
              " Document(metadata={}, page_content='i'),\n",
              " Document(metadata={}, page_content='o'),\n",
              " Document(metadata={}, page_content='n'),\n",
              " Document(metadata={}, page_content=','),\n",
              " Document(metadata={}, page_content='n'),\n",
              " Document(metadata={}, page_content='a'),\n",
              " Document(metadata={}, page_content='t'),\n",
              " Document(metadata={}, page_content='u'),\n",
              " Document(metadata={}, page_content='r'),\n",
              " Document(metadata={}, page_content='a'),\n",
              " Document(metadata={}, page_content='l'),\n",
              " Document(metadata={}, page_content='-'),\n",
              " Document(metadata={}, page_content='l'),\n",
              " Document(metadata={}, page_content='a'),\n",
              " Document(metadata={}, page_content='n'),\n",
              " Document(metadata={}, page_content='g'),\n",
              " Document(metadata={}, page_content='u'),\n",
              " Document(metadata={}, page_content='a'),\n",
              " Document(metadata={}, page_content='g'),\n",
              " Document(metadata={}, page_content='e'),\n",
              " Document(metadata={}, page_content='u'),\n",
              " Document(metadata={}, page_content='n'),\n",
              " Document(metadata={}, page_content='d'),\n",
              " Document(metadata={}, page_content='e'),\n",
              " Document(metadata={}, page_content='r'),\n",
              " Document(metadata={}, page_content='s'),\n",
              " Document(metadata={}, page_content='t'),\n",
              " Document(metadata={}, page_content='a'),\n",
              " Document(metadata={}, page_content='n'),\n",
              " Document(metadata={}, page_content='d'),\n",
              " Document(metadata={}, page_content='i'),\n",
              " Document(metadata={}, page_content='n'),\n",
              " Document(metadata={}, page_content='g'),\n",
              " Document(metadata={}, page_content=','),\n",
              " Document(metadata={}, page_content='a'),\n",
              " Document(metadata={}, page_content='n'),\n",
              " Document(metadata={}, page_content='d'),\n",
              " Document(metadata={}, page_content='n'),\n",
              " Document(metadata={}, page_content='a'),\n",
              " Document(metadata={}, page_content='t'),\n",
              " Document(metadata={}, page_content='u'),\n",
              " Document(metadata={}, page_content='r'),\n",
              " Document(metadata={}, page_content='a'),\n",
              " Document(metadata={}, page_content='l'),\n",
              " Document(metadata={}, page_content='-'),\n",
              " Document(metadata={}, page_content='l'),\n",
              " Document(metadata={}, page_content='a'),\n",
              " Document(metadata={}, page_content='n'),\n",
              " Document(metadata={}, page_content='g'),\n",
              " Document(metadata={}, page_content='u'),\n",
              " Document(metadata={}, page_content='a'),\n",
              " Document(metadata={}, page_content='g'),\n",
              " Document(metadata={}, page_content='e')]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Recursive Character splitter"
      ],
      "metadata": {
        "id": "PIT7k1h-yrLD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter"
      ],
      "metadata": {
        "id": "qzc_eCeVyYqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)"
      ],
      "metadata": {
        "id": "qQjW4woMy-67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text= text_splitter.split_documents(data)"
      ],
      "metadata": {
        "id": "eyy8TK9VzK2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "291KE_wazQEd",
        "outputId": "6b6a95ab-1856-45ef-b7dd-7f57673d9cee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "472"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfP4MChBzZpz",
        "outputId": "b3998cb9-7584-412f-ccac-0524bbfee1b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'source': '/content/impromptu-rh.pdf', 'page': 1}, page_content='Impromptu\\nAmplifying Our Humanity \\nThrough AI\\nBy Reid Hoffman  \\nwith GPT-4')"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8DWvrAIzg7k",
        "outputId": "85f40e6e-5581-49fe-815e-be3579e854f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'source': '/content/impromptu-rh.pdf', 'page': 2}, page_content='Impromptu: AmplIfyIng our HumAnIty tHrougH AI \\nby Reid Hoffman with GPT-4\\nISBNs:  979-8-9878319-1-5 Trade Paperback \\n979-8-9878319-2-2 Hardcover \\n979-8-9878319-0-8 Ebook\\nCopyright 2023 Dallepedia LLC\\nPublished by Dallepedia LLC. All rights reserved. No portion \\nof this work may be reproduced in any form, with the limited \\nexception of brief quotations in editorial reviews, without express \\nwritten permission from the publisher.')"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text[6]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzKecaXk0M0y",
        "outputId": "719d8073-83ea-4787-d934-f20d05da83c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'source': '/content/impromptu-rh.pdf', 'page': 7}, page_content='1\\nINTRODUCTION: MOMENTS OF \\nENLIGHTENMENT\\nFor Isaac Newton, it was an apple falling from a tree that \\nsupposedly led him to formulate the law of universal gravity. \\nFor Benjamin Franklin, it was a key on a kite string, struck by \\nlightning in a thunderstorm, that proved electricity could be \\ntransferred and stored.\\nMy initial “AHA!” moment regarding the current state of \\nAI came in the form of a joke. It was July 2022, and I asked \\nGPT-4, “How many restaurant inspectors does it take to change \\na lightbulb?” \\nGPT-4, as you may know, is an advanced type of AI system, or \\nnatural-language processor, known as a large language model \\n(LLM). Prompt it with a few words or sentences and it will \\ngenerate coherent and diverse texts in response. In this way, it \\ncan answer questions, perform tasks, and productively interact \\nwith its human users.\\nAlong with its predecessors, including the wildly popular \\nChatGPT, GPT-4 was developed by OpenAI, a research organi-')"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text[7]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NATw8oD80Rf7",
        "outputId": "2027783d-05a7-4449-cbc6-27e7c06d3d71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'source': '/content/impromptu-rh.pdf', 'page': 7}, page_content='ChatGPT, GPT-4 was developed by OpenAI, a research organi-\\nzation founded in 2015 with a mission to give millions of people \\ndirect, hands-on access to powerful new AI tools.')"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# semantic chunking"
      ],
      "metadata": {
        "id": "j3dWiqdR1ef4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain-community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aL7RW9Bk0VmK",
        "outputId": "fdc7b4a9-e357-48a7-88ba-591d5a88bbb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.10/dist-packages (0.3.9)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (3.11.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: httpx-sse<0.5.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.4.0)\n",
            "Requirement already satisfied: langchain<0.4.0,>=0.3.8 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.3.9)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.21 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.3.21)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.1.143)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (1.26.4)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.6.1)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (9.0.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.17.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.23.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.8->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.8->langchain-community) (2.9.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.21->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.21->langchain-community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.21->langchain-community) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-community) (0.27.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-community) (3.10.11)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (1.0.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.21->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.8->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.8->langchain-community) (2.23.4)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (1.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pypdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBRPgdfV12jM",
        "outputId": "2a327b04-e9b6-4d08-d627-007ee3731cac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.10/dist-packages (5.1.0)\n",
            "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf) (4.12.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader"
      ],
      "metadata": {
        "id": "nkJkA5qy181y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = PyPDFLoader('/content/Natural language processing.pdf')"
      ],
      "metadata": {
        "id": "Xh-VSEjA2Bhc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = loader.load()"
      ],
      "metadata": {
        "id": "9VBiBz9T2EOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter"
      ],
      "metadata": {
        "id": "-XqoCU9Y2I9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)"
      ],
      "metadata": {
        "id": "UqfRr1rH2Rji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = text_splitter.split_documents(data)"
      ],
      "metadata": {
        "id": "t71JdQES2m2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "406K1b_v2tJz",
        "outputId": "fcae1887-9fce-43d7-8726-2dc72e003dc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': '/content/Natural language processing.pdf', 'page': 0}, page_content='Natural language processing (NLP) is a subfield of computer science and \\nespecially artificial intelligence. It is primarily concerned with providing computers \\nwith the ability to process data encoded in natural language and is thus closely \\nrelated to information retrieval, knowledge representation and computational \\nlinguistics, a subfield of linguistics. Typically data is collected in text corpora, using \\neither rule-based, statistical or neural-based approaches in machine \\nlearning and deep learning. \\nMajor tasks in natural language processing are speech recognition, text \\nclassification, natural-language understanding, and natural-language generation. \\nHistory \\n[edit] \\nFurther information: History of natural language processing \\nNatural language processing has its roots in the 1950s.[1] Already in 1950, Alan \\nTuring published an article titled \"Computing Machinery and Intelligence\" which \\nproposed what is now called the Turing test as a criterion of intelligence, though at'),\n",
              " Document(metadata={'source': '/content/Natural language processing.pdf', 'page': 0}, page_content=\"proposed what is now called the Turing test as a criterion of intelligence, though at \\nthe time that was not articulated as a problem separate from artificial intelligence. \\nThe proposed test includes a task that involves the automated interpretation and \\ngeneration of natural language. \\nSymbolic NLP (1950s – early 1990s) \\n[edit] \\nThe premise of symbolic NLP is well-summarized by John Searle's Chinese \\nroom experiment: Given a collection of rules (e.g., a Chinese phrasebook, with \\nquestions and matching answers), the computer emulates natural language \\nunderstanding (or other NLP tasks) by applying those rules to the data it confronts. \\n• 1950s: The Georgetown experiment in 1954 involved fully automatic \\ntranslation of more than sixty Russian sentences into English. The authors \\nclaimed that within three or five years, machine translation would be a solved \\nproblem.[2] However, real progress was much slower, and after the ALPAC\"),\n",
              " Document(metadata={'source': '/content/Natural language processing.pdf', 'page': 0}, page_content='problem.[2] However, real progress was much slower, and after the ALPAC \\nreport in 1966, which found that ten years of research had failed to fulfill the \\nexpectations, funding for machine translation was dramatically reduced. Little \\nfurther research in machine translation was conducted in America (though some \\nresearch continued elsewhere, such as Japan and Europe[3]) until the late 1980s \\nwhen the first statistical machine translation systems were developed. \\n• 1960s: Some notably successful natural language processing systems \\ndeveloped in the 1960s were SHRDLU, a natural language system working in \\nrestricted \"blocks worlds\" with restricted vocabularies, and ELIZA, a simulation of \\na Rogerian psychotherapist, written by Joseph Weizenbaum between 1964 and \\n1966. Using almost no information about human thought or emotion, ELIZA \\nsometimes provided a startlingly human-like interaction. When the \"patient\" \\nexceeded the very small knowledge base, ELIZA might provide a generic'),\n",
              " Document(metadata={'source': '/content/Natural language processing.pdf', 'page': 0}, page_content='exceeded the very small knowledge base, ELIZA might provide a generic \\nresponse, for example, responding to \"My head hurts\" with \"Why do you say your \\nhead hurts?\". Ross Quillian\\'s successful work on natural language was \\ndemonstrated with a vocabulary of only twenty words, because that was all that \\nwould fit in a computer memory at the time.[4]'),\n",
              " Document(metadata={'source': '/content/Natural language processing.pdf', 'page': 1}, page_content='• 1970s: During the 1970s, many programmers began to write \\n\"conceptual ontologies\", which structured real-world information into computer-\\nunderstandable data. Examples are MARGIE (Schank, 1975), SAM (Cullingford, \\n1978), PAM (Wilensky, 1978), TaleSpin (Meehan, 1976), QUALM (Lehnert, \\n1977), Politics (Carbonell, 1979), and Plot Units (Lehnert 1981). During this time, \\nthe first chatterbots were written (e.g., PARRY). \\n• 1980s: The 1980s and early 1990s mark the heyday of symbolic methods in \\nNLP. Focus areas of the time included research on rule-based parsing (e.g., the \\ndevelopment of HPSG as a computational operationalization of generative \\ngrammar), morphology (e.g., two-level morphology[5]), semantics (e.g., Lesk \\nalgorithm), reference (e.g., within Centering Theory[6]) and other areas of natural \\nlanguage understanding (e.g., in the Rhetorical Structure Theory). Other lines of \\nresearch were continued, e.g., the development of chatterbots'),\n",
              " Document(metadata={'source': '/content/Natural language processing.pdf', 'page': 1}, page_content=\"research were continued, e.g., the development of chatterbots \\nwith Racter and Jabberwacky. An important development (that eventually led to \\nthe statistical turn in the 1990s) was the rising importance of quantitative \\nevaluation in this period.[7] \\nStatistical NLP (1990s–2010s) \\n[edit] \\nUp until the 1980s, most natural language processing systems were based on \\ncomplex sets of hand-written rules. Starting in the late 1980s, however, there was a \\nrevolution in natural language processing with the introduction of machine \\nlearning algorithms for language processing. This was due to both the steady \\nincrease in computational power (see Moore's law) and the gradual lessening of the \\ndominance of Chomskyan theories of linguistics (e.g. transformational grammar), \\nwhose theoretical underpinnings discouraged the sort of corpus linguistics that \\nunderlies the machine-learning approach to language processing.[8] \\n• 1990s: Many of the notable early successes in statistical methods in NLP\"),\n",
              " Document(metadata={'source': '/content/Natural language processing.pdf', 'page': 1}, page_content='• 1990s: Many of the notable early successes in statistical methods in NLP \\noccurred in the field of machine translation, due especially to work at IBM \\nResearch, such as IBM alignment models. These systems were able to take \\nadvantage of existing multilingual textual corpora that had been produced by \\nthe Parliament of Canada and the European Union as a result of laws calling for \\nthe translation of all governmental proceedings into all official languages of the \\ncorresponding systems of government. However, most other systems depended \\non corpora specifically developed for the tasks implemented by these systems, \\nwhich was (and often continues to be) a major limitation in the success of these \\nsystems. As a result, a great deal of research has gone into methods of more \\neffectively learning from limited amounts of data. \\n• 2000s: With the growth of the web, increasing amounts of raw (unannotated) \\nlanguage data have become available since the mid-1990s. Research has thus'),\n",
              " Document(metadata={'source': '/content/Natural language processing.pdf', 'page': 1}, page_content='language data have become available since the mid-1990s. Research has thus \\nincreasingly focused on unsupervised and semi-supervised learning algorithms. \\nSuch algorithms can learn from data that has not been hand-annotated with the \\ndesired answers or using a combination of annotated and non-annotated data. \\nGenerally, this task is much more difficult than supervised learning, and typically \\nproduces less accurate results for a given amount of input data. However, there \\nis an enormous amount of non-annotated data available (including, among other \\nthings, the entire content of the World Wide Web), which can often make up for \\nthe inferior results if the algorithm used has a low enough time complexity to be \\npractical.'),\n",
              " Document(metadata={'source': '/content/Natural language processing.pdf', 'page': 2}, page_content='Neural NLP (present) \\n[edit] \\nIn 2003, word n-gram model, at the time the best statistical algorithm, was \\noutperformed by a multi-layer perceptron (with a single hidden layer and context \\nlength of several words trained on up to 14 million of words with a CPU cluster \\nin language modelling) by Yoshua Bengio with co-authors.[9] \\nIn 2010, Tomáš Mikolov (then a PhD student at Brno University of Technology) with \\nco-authors applied a simple recurrent neural network with a single hidden layer to \\nlanguage modelling,[10] and in the following years he went on to develop Word2vec. In \\nthe 2010s, representation learning and deep neural network-style (featuring many \\nhidden layers) machine learning methods became widespread in natural language \\nprocessing. That popularity was due partly to a flurry of results showing that such \\ntechniques[11][12] can achieve state-of-the-art results in many natural language tasks,'),\n",
              " Document(metadata={'source': '/content/Natural language processing.pdf', 'page': 2}, page_content='techniques[11][12] can achieve state-of-the-art results in many natural language tasks, \\ne.g., in language modeling[13] and parsing.[14][15] This is increasingly important in \\nmedicine and healthcare, where NLP helps analyze notes and text in electronic \\nhealth records that would otherwise be inaccessible for study when seeking to \\nimprove care[16] or protect patient privacy.[17] \\nApproaches: Symbolic, statistical, neural \\nnetworks \\n[edit] \\nSymbolic approach, i.e., the hand-coding of a set of rules for manipulating symbols, \\ncoupled with a dictionary lookup, was historically the first approach used both by AI \\nin general and by NLP in particular:[18][19] such as by writing grammars or devising \\nheuristic rules for stemming. \\nMachine learning approaches, which include both statistical and neural networks, on \\nthe other hand, have many advantages over the symbolic approach: \\n• both statistical and neural networks methods can focus more on the most'),\n",
              " Document(metadata={'source': '/content/Natural language processing.pdf', 'page': 2}, page_content='• both statistical and neural networks methods can focus more on the most \\ncommon cases extracted from a corpus of texts, whereas the rule-based \\napproach needs to provide rules for both rare cases and common ones equally. \\n• language models, produced by either statistical or neural networks methods, are \\nmore robust to both unfamiliar (e.g. containing words or structures that have not \\nbeen seen before) and erroneous input (e.g. with misspelled words or words \\naccidentally omitted) in comparison to the rule-based systems, which are also \\nmore costly to produce. \\n• the larger such a (probabilistic) language model is, the more accurate it \\nbecomes, in contrast to rule-based systems that can gain accuracy only by \\nincreasing the amount and complexity of the rules leading \\nto intractability problems. \\nAlthough rule-based systems for manipulating symbols were still in use in 2020, they \\nhave become mostly obsolete with the advance of LLMs in 2023. \\nBefore that they were commonly used:'),\n",
              " Document(metadata={'source': '/content/Natural language processing.pdf', 'page': 3}, page_content='• when the amount of training data is insufficient to successfully apply machine \\nlearning methods, e.g., for the machine translation of low-resource languages \\nsuch as provided by the Apertium system, \\n• for preprocessing in NLP pipelines, e.g., tokenization, or \\n• for postprocessing and transforming the output of NLP pipelines, e.g., \\nfor knowledge extraction from syntactic parses. \\nStatistical approach \\n[edit] \\nIn the late 1980s and mid-1990s, the statistical approach ended a period of AI winter, \\nwhich was caused by the inefficiencies of the rule-based approaches.[20][21] \\nThe earliest decision trees, producing systems of hard if–then rules, were still very \\nsimilar to the old rule-based approaches. Only the introduction of hidden Markov \\nmodels, applied to part-of-speech tagging, announced the end of the old rule-based \\napproach.')]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install fastembed"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LwZviqDQ2vlJ",
        "outputId": "6b18f0c7-ce04-42a4-a14a-536b2a1fb45f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fastembed in /usr/local/lib/python3.10/dist-packages (0.4.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.20 in /usr/local/lib/python3.10/dist-packages (from fastembed) (0.26.2)\n",
            "Requirement already satisfied: loguru<0.8.0,>=0.7.2 in /usr/local/lib/python3.10/dist-packages (from fastembed) (0.7.2)\n",
            "Requirement already satisfied: mmh3<5.0.0,>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from fastembed) (4.1.0)\n",
            "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.10/dist-packages (from fastembed) (1.26.4)\n",
            "Requirement already satisfied: onnx<2.0.0,>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from fastembed) (1.17.0)\n",
            "Requirement already satisfied: onnxruntime<1.20.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from fastembed) (1.19.2)\n",
            "Requirement already satisfied: pillow<11.0.0,>=10.3.0 in /usr/local/lib/python3.10/dist-packages (from fastembed) (10.4.0)\n",
            "Requirement already satisfied: py-rust-stemmers<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from fastembed) (0.1.3)\n",
            "Requirement already satisfied: requests<3.0,>=2.31 in /usr/local/lib/python3.10/dist-packages (from fastembed) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<1.0,>=0.15 in /usr/local/lib/python3.10/dist-packages (from fastembed) (0.20.3)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.66 in /usr/local/lib/python3.10/dist-packages (from fastembed) (4.66.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.20->fastembed) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.20->fastembed) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.20->fastembed) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.20->fastembed) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.20->fastembed) (4.12.2)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx<2.0.0,>=1.15.0->fastembed) (4.25.5)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime<1.20.0,>=1.17.0->fastembed) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime<1.20.0,>=1.17.0->fastembed) (24.3.25)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime<1.20.0,>=1.17.0->fastembed) (1.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.31->fastembed) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.31->fastembed) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.31->fastembed) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.31->fastembed) (2024.8.30)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime<1.20.0,>=1.17.0->fastembed) (10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime<1.20.0,>=1.17.0->fastembed) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.embeddings import FastEmbedEmbeddings"
      ],
      "metadata": {
        "id": "32jrF3Uk6kw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = FastEmbedEmbeddings(model_name=\"BAAI/bge-base-en-v1.5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173,
          "referenced_widgets": [
            "624af38881d6414dad5a9c1cfac60af5",
            "6f4354752bea43b1a51cb8e7a31f42d1",
            "25e25d892910466d9c09c350b5613770",
            "492762651a6140dfb99e80361d4a64ec",
            "c0d3a52ce0974913a7b8921cab6b42b8",
            "63d3eded03ac4babb9b717e41cf417d4",
            "22e2f1b735af445085c2562585882c01",
            "6cede6e114a6464aa921957c2d617bb6",
            "c77f9b882188477f8b4cfa57dc380227",
            "2f4386e18d044cb496b832defa8e7cfd",
            "0a48cb63e7534630907c17854f605b04"
          ]
        },
        "id": "7HJZec22694w",
        "outputId": "9d9a046f-9ea5-4af0-86b0-6e22a2582db1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "624af38881d6414dad5a9c1cfac60af5"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install faiss-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tqcwpKYZ7aIf",
        "outputId": "42046085-a5dd-4237-cc5c-fc70cd463d34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.10/dist-packages (1.9.0.post1)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import FAISS"
      ],
      "metadata": {
        "id": "9ZEKOzNw73Nf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "db = FAISS.from_documents(text, embeddings)"
      ],
      "metadata": {
        "id": "Jm43sboU794m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkQnl9rB_EyU",
        "outputId": "c4df1465-243f-4e03-aedf-63426e9fb848"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})"
      ],
      "metadata": {
        "id": "JcJNHOcS9ien"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"what is NLP?\""
      ],
      "metadata": {
        "id": "XJUE8-oH_m58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retrieved_docs = retriever.get_relevant_documents(query)"
      ],
      "metadata": {
        "id": "RL4w7nYW_qXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retrieved_docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ArL_2t1y_zMy",
        "outputId": "94d600c3-3f46-49ee-9ccc-cd4eeb403649"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': '/content/Natural language processing.pdf', 'page': 0}, page_content='Natural language processing (NLP) is a subfield of computer science and \\nespecially artificial intelligence. It is primarily concerned with providing computers \\nwith the ability to process data encoded in natural language and is thus closely \\nrelated to information retrieval, knowledge representation and computational \\nlinguistics, a subfield of linguistics. Typically data is collected in text corpora, using \\neither rule-based, statistical or neural-based approaches in machine \\nlearning and deep learning. \\nMajor tasks in natural language processing are speech recognition, text \\nclassification, natural-language understanding, and natural-language generation. \\nHistory \\n[edit] \\nFurther information: History of natural language processing \\nNatural language processing has its roots in the 1950s.[1] Already in 1950, Alan \\nTuring published an article titled \"Computing Machinery and Intelligence\" which \\nproposed what is now called the Turing test as a criterion of intelligence, though at'),\n",
              " Document(metadata={'source': '/content/Natural language processing.pdf', 'page': 2}, page_content='techniques[11][12] can achieve state-of-the-art results in many natural language tasks, \\ne.g., in language modeling[13] and parsing.[14][15] This is increasingly important in \\nmedicine and healthcare, where NLP helps analyze notes and text in electronic \\nhealth records that would otherwise be inaccessible for study when seeking to \\nimprove care[16] or protect patient privacy.[17] \\nApproaches: Symbolic, statistical, neural \\nnetworks \\n[edit] \\nSymbolic approach, i.e., the hand-coding of a set of rules for manipulating symbols, \\ncoupled with a dictionary lookup, was historically the first approach used both by AI \\nin general and by NLP in particular:[18][19] such as by writing grammars or devising \\nheuristic rules for stemming. \\nMachine learning approaches, which include both statistical and neural networks, on \\nthe other hand, have many advantages over the symbolic approach: \\n• both statistical and neural networks methods can focus more on the most'),\n",
              " Document(metadata={'source': '/content/Natural language processing.pdf', 'page': 1}, page_content='• 1990s: Many of the notable early successes in statistical methods in NLP \\noccurred in the field of machine translation, due especially to work at IBM \\nResearch, such as IBM alignment models. These systems were able to take \\nadvantage of existing multilingual textual corpora that had been produced by \\nthe Parliament of Canada and the European Union as a result of laws calling for \\nthe translation of all governmental proceedings into all official languages of the \\ncorresponding systems of government. However, most other systems depended \\non corpora specifically developed for the tasks implemented by these systems, \\nwhich was (and often continues to be) a major limitation in the success of these \\nsystems. As a result, a great deal of research has gone into methods of more \\neffectively learning from limited amounts of data. \\n• 2000s: With the growth of the web, increasing amounts of raw (unannotated) \\nlanguage data have become available since the mid-1990s. Research has thus')]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install groq"
      ],
      "metadata": {
        "id": "T7GS5gHu_5Ci",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e32d518-c01d-4b04-c5ef-cf667a26cf39"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting groq\n",
            "  Downloading groq-0.13.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from groq) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from groq) (2.10.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.10/dist-packages (from groq) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq) (2.27.1)\n",
            "Downloading groq-0.13.1-py3-none-any.whl (109 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.1/109.1 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: groq\n",
            "Successfully installed groq-0.13.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-groq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mM31MZFrbZO2",
        "outputId": "367cb320-5520-4575-f558-fbf7d3d8c223"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-groq\n",
            "  Downloading langchain_groq-0.2.2-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: groq<1,>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from langchain-groq) (0.13.1)\n",
            "Collecting langchain-core<0.4.0,>=0.3.27 (from langchain-groq)\n",
            "  Downloading langchain_core-0.3.28-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain-groq) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain-groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain-groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain-groq) (2.10.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain-groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain-groq) (4.12.2)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain-groq) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain-groq) (1.33)\n",
            "Requirement already satisfied: langsmith<0.3,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain-groq) (0.2.3)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain-groq) (24.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain-groq) (9.0.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain-groq) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain-groq) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.27->langchain-groq) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain-groq) (3.10.12)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain-groq) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain-groq) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain-groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain-groq) (2.27.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain-groq) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain-groq) (2.2.3)\n",
            "Downloading langchain_groq-0.2.2-py3-none-any.whl (14 kB)\n",
            "Downloading langchain_core-0.3.28-py3-none-any.whl (411 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.6/411.6 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: langchain-core, langchain-groq\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.25\n",
            "    Uninstalling langchain-core-0.3.25:\n",
            "      Successfully uninstalled langchain-core-0.3.25\n",
            "Successfully installed langchain-core-0.3.28 langchain-groq-0.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from groq import Groq"
      ],
      "metadata": {
        "id": "QAtNnpA-b5Hx"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-dotenv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_6j96icab_S4",
        "outputId": "8644ee52-0712-47a4-adbd-adb7afc570f1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv\n",
        "load_dotenv()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQC0MqtscDyz",
        "outputId": "24204021-2291-4b22-e88f-a84fef853ca7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('.env', 'w') as f:\n",
        "  f.write('GROQ_API_KEY=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')"
      ],
      "metadata": {
        "id": "Uqj0wOMBcH2r"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq"
      ],
      "metadata": {
        "id": "CqLLSjq1cQ99"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA"
      ],
      "metadata": {
        "id": "6nKHUi07cd6D"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate"
      ],
      "metadata": {
        "id": "qBdbYxBycjar"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template ='''\n",
        "You are an helpful assistant.Greet the user before answering..\n",
        "\n",
        "{context}\n",
        "{question}\n",
        "'''"
      ],
      "metadata": {
        "id": "-aSaQZXhcm2a"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = PromptTemplate(template =prompt_template,input_variables=['context','question'] )"
      ],
      "metadata": {
        "id": "qgJcg4Ijc4cc"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationalRetrievalChain"
      ],
      "metadata": {
        "id": "ovDhpPcMdgKd"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dgXwoMqdniv",
        "outputId": "c4af9a6f-4925-448f-c35e-0cd50d64bf6b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-25-4ed970f367f8>:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferMemory(memory_key='chat_history', return_messages=False)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AfDZBMeFdxYs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}